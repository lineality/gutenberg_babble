### gutenberg_babble

Experiments in language generation, using books from https://gutenberg.org/ ,
starting from basic older models and moving to newer models and other experiments.

## Including
1. RNN models (1990's)
 - https://en.wikipedia.org/wiki/Recurrent_neural_network
2. LSTM models (2015-2019)
 - https://en.wikipedia.org/wiki/Long_short-term_memory
3. Gemma Model (2025, Google ~open source model
 - https://en.wikipedia.org/wiki/Gemma_(language_model),
 - https://developers.googleblog.com/en/introducing-gemma-3-270m/
 - https://huggingface.co/google/gemma-3-270m
4. Byte Tokenizer (developed here)
 - raw files bytes are tokens
5. Perseids Model-Family (developed here)
6. Task-Reward Validation-Loss-Suite Experiments


Also see:
- https://github.com/rasbt
- https://github.com/rasbt/LLMs-from-scratch
