{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe5651fb-30fa-470c-ac7f-3f05ae6bfaed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nGutenberg Text Generation using Recurrent Neural Network (RNN)\\n\\nThis module implements a character-level RNN for generating Gutenberg-like text.\\nThe implementation follows production-safe coding practices with extensive error handling,\\ndocumentation, and clear variable naming conventions.\\n\\nAuthor: AI Assistant\\nDate: 2024\\nPurpose: Educational demonstration of basic RNN text generation\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Gutenberg Text Generation using Recurrent Neural Network (RNN)\n",
    "\n",
    "This module implements a character-level RNN for generating Gutenberg-like text.\n",
    "The implementation follows production-safe coding practices with extensive error handling,\n",
    "documentation, and clear variable naming conventions.\n",
    "\n",
    "Author: AI Assistant\n",
    "Date: 2024\n",
    "Purpose: Educational demonstration of basic RNN text generation\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9accf6b6-43c6-4286-b877-a5dbbea1053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import traceback\n",
    "import logging\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbf69463-bee7-4785-96f1-04b3d46935c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configure logging for production safety\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Suppress warnings for cleaner output (production consideration)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de00af0b-0ea4-4330-8de0-47c7a4ebbf76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bee86862-711f-40f4-a178-3a942a076a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class GutenbergTextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset class for handling Gutenberg text data.\n",
    "\n",
    "    This class converts raw text into character-level sequences suitable for training\n",
    "    an RNN model. It creates input-target pairs where the target is the next character\n",
    "    in the sequence.\n",
    "\n",
    "    Attributes:\n",
    "        raw_text_content (str): The original text content\n",
    "        unique_characters_sorted (List[str]): Sorted list of unique characters\n",
    "        character_to_index_mapping (Dict[str, int]): Maps characters to indices\n",
    "        index_to_character_mapping (Dict[int, str]): Maps indices back to characters\n",
    "        sequence_length_for_training (int): Length of input sequences\n",
    "        encoded_text_as_integers (List[int]): Text converted to integer indices\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, text_file_path: str, sequence_length_for_training: int = 100):\n",
    "        \"\"\"\n",
    "        Initialize the Gutenberg text dataset.\n",
    "\n",
    "        Args:\n",
    "            text_file_path (str): Path to the Gutenberg text file\n",
    "            sequence_length_for_training (int): Length of sequences for training\n",
    "\n",
    "        Raises:\n",
    "            FileNotFoundError: If the specified text file doesn't exist\n",
    "            ValueError: If sequence length is invalid\n",
    "            Exception: For any other unexpected errors during initialization\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Initializing Gutenberg dataset from: {text_file_path}\")\n",
    "\n",
    "            # Validate input parameters\n",
    "            if sequence_length_for_training <= 0:\n",
    "                raise ValueError(\n",
    "                    \"sequence_length_for_training must be positive integer\"\n",
    "                )\n",
    "\n",
    "            self.sequence_length_for_training = sequence_length_for_training\n",
    "\n",
    "            # Read and validate text file\n",
    "            text_file_path_object = Path(text_file_path)\n",
    "            if not text_file_path_object.exists():\n",
    "                raise FileNotFoundError(f\"Text file not found: {text_file_path}\")\n",
    "\n",
    "            # Read text content with error handling\n",
    "            self.raw_text_content = self._safely_read_text_file(text_file_path_object)\n",
    "\n",
    "            # Create character mappings\n",
    "            self._create_character_mappings()\n",
    "\n",
    "            # Encode text as integers\n",
    "            self._encode_text_to_integers()\n",
    "\n",
    "            logger.info(\n",
    "                f\"Dataset initialized successfully. Text length: {len(self.raw_text_content)}, \"\n",
    "                f\"Unique characters: {len(self.unique_characters_sorted)}\"\n",
    "            )\n",
    "\n",
    "        except FileNotFoundError as file_error:\n",
    "            logger.error(\n",
    "                f\"File not found error during dataset initialization: {file_error}\"\n",
    "            )\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "        except ValueError as value_error:\n",
    "            logger.error(f\"Value error during dataset initialization: {value_error}\")\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "        except Exception as unexpected_error:\n",
    "            logger.error(\n",
    "                f\"Unexpected error during dataset initialization: {unexpected_error}\"\n",
    "            )\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "\n",
    "    def _safely_read_text_file(self, text_file_path_object: Path) -> str:\n",
    "        \"\"\"\n",
    "        Safely read text file with multiple encoding attempts.\n",
    "\n",
    "        Args:\n",
    "            text_file_path_object (Path): Path object for the text file\n",
    "\n",
    "        Returns:\n",
    "            str: Content of the text file\n",
    "\n",
    "        Raises:\n",
    "            Exception: If file cannot be read with any encoding\n",
    "        \"\"\"\n",
    "        encodings_to_try = [\"utf-8\", \"latin-1\", \"cp1252\", \"ascii\"]\n",
    "\n",
    "        for encoding_attempt in encodings_to_try:\n",
    "            try:\n",
    "                logger.info(\n",
    "                    f\"Attempting to read file with encoding: {encoding_attempt}\"\n",
    "                )\n",
    "                with open(\n",
    "                    text_file_path_object, \"r\", encoding=encoding_attempt\n",
    "                ) as file_handle:\n",
    "                    text_content = file_handle.read()\n",
    "                logger.info(f\"Successfully read file with encoding: {encoding_attempt}\")\n",
    "                return text_content\n",
    "            except UnicodeDecodeError as decode_error:\n",
    "                logger.warning(\n",
    "                    f\"Failed to read with encoding {encoding_attempt}: {decode_error}\"\n",
    "                )\n",
    "                continue\n",
    "            except Exception as read_error:\n",
    "                logger.error(\n",
    "                    f\"Unexpected error reading file with {encoding_attempt}: {read_error}\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "        raise Exception(\n",
    "            f\"Could not read file with any of these encodings: {encodings_to_try}\"\n",
    "        )\n",
    "\n",
    "    def _create_character_mappings(self) -> None:\n",
    "        \"\"\"\n",
    "        Create bidirectional mappings between characters and integer indices.\n",
    "\n",
    "        This method creates dictionaries to convert between characters and their\n",
    "        corresponding integer indices for neural network processing.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Get unique characters and sort them for consistency\n",
    "            unique_characters_set = set(self.raw_text_content)\n",
    "            self.unique_characters_sorted = sorted(list(unique_characters_set))\n",
    "\n",
    "            # Create character to index mapping\n",
    "            self.character_to_index_mapping = {\n",
    "                character: index\n",
    "                for index, character in enumerate(self.unique_characters_sorted)\n",
    "            }\n",
    "\n",
    "            # Create index to character mapping (reverse mapping)\n",
    "            self.index_to_character_mapping = {\n",
    "                index: character\n",
    "                for character, index in self.character_to_index_mapping.items()\n",
    "            }\n",
    "\n",
    "            logger.info(\n",
    "                f\"Created character mappings for {len(self.unique_characters_sorted)} unique characters\"\n",
    "            )\n",
    "\n",
    "        except Exception as mapping_error:\n",
    "            logger.error(f\"Error creating character mappings: {mapping_error}\")\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "\n",
    "    def _encode_text_to_integers(self) -> None:\n",
    "        \"\"\"\n",
    "        Convert text content to integer sequence using character mappings.\n",
    "\n",
    "        Raises:\n",
    "            KeyError: If a character is not found in the mapping\n",
    "            Exception: For any other unexpected errors\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.encoded_text_as_integers = []\n",
    "\n",
    "            for character in self.raw_text_content:\n",
    "                if character not in self.character_to_index_mapping:\n",
    "                    logger.warning(f\"Character not found in mapping: {repr(character)}\")\n",
    "                    continue\n",
    "\n",
    "                character_index = self.character_to_index_mapping[character]\n",
    "                self.encoded_text_as_integers.append(character_index)\n",
    "\n",
    "            logger.info(\n",
    "                f\"Encoded text to {len(self.encoded_text_as_integers)} integers\"\n",
    "            )\n",
    "\n",
    "        except Exception as encoding_error:\n",
    "            logger.error(f\"Error encoding text to integers: {encoding_error}\")\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Return the number of training samples available.\n",
    "\n",
    "        Returns:\n",
    "            int: Number of possible training sequences\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Calculate number of possible sequences\n",
    "            possible_sequences = (\n",
    "                len(self.encoded_text_as_integers) - self.sequence_length_for_training\n",
    "            )\n",
    "            return max(0, possible_sequences)\n",
    "        except Exception as length_error:\n",
    "            logger.error(f\"Error calculating dataset length: {length_error}\")\n",
    "            return 0\n",
    "\n",
    "    def __getitem__(self, sequence_index: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Get a single training sample (input sequence and target).\n",
    "\n",
    "        Args:\n",
    "            sequence_index (int): Index of the sequence to retrieve\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]: Input sequence and target character\n",
    "\n",
    "        Raises:\n",
    "            IndexError: If sequence_index is out of range\n",
    "            Exception: For any other unexpected errors\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Validate sequence index\n",
    "            if sequence_index < 0 or sequence_index >= len(self):\n",
    "                raise IndexError(\n",
    "                    f\"Sequence index {sequence_index} out of range [0, {len(self)})\"\n",
    "                )\n",
    "\n",
    "            # Extract input sequence\n",
    "            sequence_start_index = sequence_index\n",
    "            sequence_end_index = sequence_index + self.sequence_length_for_training\n",
    "            input_sequence_integers = self.encoded_text_as_integers[\n",
    "                sequence_start_index:sequence_end_index\n",
    "            ]\n",
    "\n",
    "            # Extract target (next character)\n",
    "            target_character_index = self.encoded_text_as_integers[sequence_end_index]\n",
    "\n",
    "            # Convert to tensors\n",
    "            input_sequence_tensor = torch.tensor(\n",
    "                input_sequence_integers, dtype=torch.long\n",
    "            )\n",
    "            target_character_tensor = torch.tensor(\n",
    "                target_character_index, dtype=torch.long\n",
    "            )\n",
    "\n",
    "            return input_sequence_tensor, target_character_tensor\n",
    "\n",
    "        except IndexError as index_error:\n",
    "            logger.error(f\"Index error in __getitem__: {index_error}\")\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "        except Exception as getitem_error:\n",
    "            logger.error(f\"Unexpected error in __getitem__: {getitem_error}\")\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "\n",
    "    def get_vocabulary_size(self) -> int:\n",
    "        \"\"\"\n",
    "        Get the size of the character vocabulary.\n",
    "\n",
    "        Returns:\n",
    "            int: Number of unique characters in the dataset\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return len(self.unique_characters_sorted)\n",
    "        except Exception as vocab_error:\n",
    "            logger.error(f\"Error getting vocabulary size: {vocab_error}\")\n",
    "            return 0\n",
    "\n",
    "    def decode_integer_sequence_to_text(self, integer_sequence: List[int]) -> str:\n",
    "        \"\"\"\n",
    "        Convert a sequence of integers back to text.\n",
    "\n",
    "        Args:\n",
    "            integer_sequence (List[int]): Sequence of character indices\n",
    "\n",
    "        Returns:\n",
    "            str: Decoded text string\n",
    "\n",
    "        Raises:\n",
    "            KeyError: If an integer index is not found in the mapping\n",
    "            Exception: For any other unexpected errors\n",
    "        \"\"\"\n",
    "        try:\n",
    "            decoded_characters = []\n",
    "\n",
    "            for integer_index in integer_sequence:\n",
    "                if integer_index not in self.index_to_character_mapping:\n",
    "                    logger.warning(\n",
    "                        f\"Integer index not found in mapping: {integer_index}\"\n",
    "                    )\n",
    "                    decoded_characters.append(\n",
    "                        \"?\"\n",
    "                    )  # Use placeholder for unknown indices\n",
    "                    continue\n",
    "\n",
    "                character = self.index_to_character_mapping[integer_index]\n",
    "                decoded_characters.append(character)\n",
    "\n",
    "            decoded_text = \"\".join(decoded_characters)\n",
    "            return decoded_text\n",
    "\n",
    "        except Exception as decode_error:\n",
    "            logger.error(f\"Error decoding integer sequence to text: {decode_error}\")\n",
    "            traceback.print_exc()\n",
    "            return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48f76ae9-f02c-4397-bc54-c733f30ac63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GutenbergRNNModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Recurrent Neural Network model for Gutenberg text generation.\n",
    "\n",
    "    This class implements a simple RNN architecture with embedding layer,\n",
    "    RNN hidden layers, and output projection for character-level text generation.\n",
    "\n",
    "    Attributes:\n",
    "        vocabulary_size_for_embeddings (int): Size of character vocabulary\n",
    "        embedding_dimension_size (int): Dimension of character embeddings\n",
    "        hidden_state_dimension_size (int): Dimension of RNN hidden states\n",
    "        number_of_rnn_layers (int): Number of stacked RNN layers\n",
    "        character_embedding_layer (nn.Embedding): Converts indices to embeddings\n",
    "        recurrent_neural_network_layers (nn.RNN): RNN processing layers\n",
    "        output_projection_layer (nn.Linear): Projects hidden states to vocabulary\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocabulary_size_for_embeddings: int,\n",
    "        embedding_dimension_size: int = 128,\n",
    "        hidden_state_dimension_size: int = 256,\n",
    "        number_of_rnn_layers: int = 2,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the Gutenberg RNN model.\n",
    "\n",
    "        Args:\n",
    "            vocabulary_size_for_embeddings (int): Size of the character vocabulary\n",
    "            embedding_dimension_size (int): Dimension of character embeddings\n",
    "            hidden_state_dimension_size (int): Dimension of RNN hidden states\n",
    "            number_of_rnn_layers (int): Number of stacked RNN layers\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If any parameter is invalid\n",
    "            Exception: For any other initialization errors\n",
    "        \"\"\"\n",
    "        try:\n",
    "            super(GutenbergRNNModel, self).__init__()\n",
    "\n",
    "            logger.info(\"Initializing Gutenberg RNN model\")\n",
    "\n",
    "            # Validate input parameters\n",
    "            self._validate_model_parameters(\n",
    "                vocabulary_size_for_embeddings,\n",
    "                embedding_dimension_size,\n",
    "                hidden_state_dimension_size,\n",
    "                number_of_rnn_layers,\n",
    "            )\n",
    "\n",
    "            # Store model configuration\n",
    "            self.vocabulary_size_for_embeddings = vocabulary_size_for_embeddings\n",
    "            self.embedding_dimension_size = embedding_dimension_size\n",
    "            self.hidden_state_dimension_size = hidden_state_dimension_size\n",
    "            self.number_of_rnn_layers = number_of_rnn_layers\n",
    "\n",
    "            # Initialize model layers\n",
    "            self._initialize_model_layers()\n",
    "\n",
    "            logger.info(\n",
    "                f\"Model initialized - Vocab: {vocabulary_size_for_embeddings}, \"\n",
    "                f\"Embed: {embedding_dimension_size}, Hidden: {hidden_state_dimension_size}, \"\n",
    "                f\"Layers: {number_of_rnn_layers}\"\n",
    "            )\n",
    "\n",
    "        except ValueError as value_error:\n",
    "            logger.error(f\"Value error during model initialization: {value_error}\")\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "        except Exception as init_error:\n",
    "            logger.error(f\"Unexpected error during model initialization: {init_error}\")\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "\n",
    "    def _validate_model_parameters(\n",
    "        self,\n",
    "        vocabulary_size_for_embeddings: int,\n",
    "        embedding_dimension_size: int,\n",
    "        hidden_state_dimension_size: int,\n",
    "        number_of_rnn_layers: int,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Validate model initialization parameters.\n",
    "\n",
    "        Args:\n",
    "            vocabulary_size_for_embeddings (int): Size of vocabulary\n",
    "            embedding_dimension_size (int): Embedding dimension\n",
    "            hidden_state_dimension_size (int): Hidden state dimension\n",
    "            number_of_rnn_layers (int): Number of RNN layers\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If any parameter is invalid\n",
    "        \"\"\"\n",
    "        if vocabulary_size_for_embeddings <= 0:\n",
    "            raise ValueError(\"vocabulary_size_for_embeddings must be positive\")\n",
    "        if embedding_dimension_size <= 0:\n",
    "            raise ValueError(\"embedding_dimension_size must be positive\")\n",
    "        if hidden_state_dimension_size <= 0:\n",
    "            raise ValueError(\"hidden_state_dimension_size must be positive\")\n",
    "        if number_of_rnn_layers <= 0:\n",
    "            raise ValueError(\"number_of_rnn_layers must be positive\")\n",
    "\n",
    "    # rnn style\n",
    "    def _initialize_model_layers(self) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the neural network layers.\n",
    "\n",
    "        Raises:\n",
    "            Exception: If layer initialization fails\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Character embedding layer\n",
    "            self.character_embedding_layer = nn.Embedding(\n",
    "                num_embeddings=self.vocabulary_size_for_embeddings,\n",
    "                embedding_dim=self.embedding_dimension_size,\n",
    "            )\n",
    "\n",
    "            # Recurrent neural network layers\n",
    "            self.recurrent_neural_network_layers = nn.RNN(\n",
    "                input_size=self.embedding_dimension_size,\n",
    "                hidden_size=self.hidden_state_dimension_size,\n",
    "                num_layers=self.number_of_rnn_layers,\n",
    "                batch_first=True,\n",
    "                dropout=0.2 if self.number_of_rnn_layers > 1 else 0.0,\n",
    "            )\n",
    "\n",
    "            # Output projection layer\n",
    "            self.output_projection_layer = nn.Linear(\n",
    "                in_features=self.hidden_state_dimension_size,\n",
    "                out_features=self.vocabulary_size_for_embeddings,\n",
    "            )\n",
    "\n",
    "            logger.info(\"Model layers initialized successfully\")\n",
    "\n",
    "        except Exception as layer_error:\n",
    "            logger.error(f\"Error initializing model layers: {layer_error}\")\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "    \n",
    "    # # LSTM style\n",
    "    # def _initialize_model_layers(self) -> None:\n",
    "    #     \"\"\"\n",
    "    #     Initialize the neural network layers with LSTM instead of RNN.\n",
    "        \n",
    "    #     Raises:\n",
    "    #         Exception: If layer initialization fails\n",
    "    #     \"\"\"\n",
    "    #     try:\n",
    "    #         # Character embedding layer (UNCHANGED)\n",
    "    #         self.character_embedding_layer = nn.Embedding(\n",
    "    #             num_embeddings=self.vocabulary_size_for_embeddings,\n",
    "    #             embedding_dim=self.embedding_dimension_size\n",
    "    #         )\n",
    "            \n",
    "    #         # LSTM layers instead of RNN layers (ONLY CHANGE NEEDED)\n",
    "    #         self.recurrent_neural_network_layers = nn.LSTM(  # Changed from nn.RNN to nn.LSTM\n",
    "    #             input_size=self.embedding_dimension_size,\n",
    "    #             hidden_size=self.hidden_state_dimension_size,\n",
    "    #             num_layers=self.number_of_rnn_layers,\n",
    "    #             batch_first=True,\n",
    "    #             dropout=0.2 if self.number_of_rnn_layers > 1 else 0.0\n",
    "    #         )\n",
    "            \n",
    "    #         # Output projection layer (UNCHANGED)\n",
    "    #         self.output_projection_layer = nn.Linear(\n",
    "    #             in_features=self.hidden_state_dimension_size,\n",
    "    #             out_features=self.vocabulary_size_for_embeddings\n",
    "    #         )\n",
    "            \n",
    "    #         logger.info(\"Model layers initialized successfully with LSTM\")\n",
    "            \n",
    "    #     except Exception as layer_error:\n",
    "    #         logger.error(f\"Error initializing model layers: {layer_error}\")\n",
    "    #         traceback.print_exc()\n",
    "    #         raise\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_character_indices: torch.Tensor,\n",
    "        hidden_state_tensor: Optional[torch.Tensor] = None,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass through the RNN model.\n",
    "\n",
    "        Args:\n",
    "            input_character_indices (torch.Tensor): Input character indices [batch_size, seq_len]\n",
    "            hidden_state_tensor (Optional[torch.Tensor]): Initial hidden state\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]: Output logits and final hidden state\n",
    "\n",
    "        Raises:\n",
    "            RuntimeError: If forward pass fails\n",
    "            Exception: For any other unexpected errors\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Get batch size and sequence length\n",
    "            batch_size, sequence_length = input_character_indices.size()\n",
    "\n",
    "            # Convert character indices to embeddings\n",
    "            character_embeddings = self.character_embedding_layer(\n",
    "                input_character_indices\n",
    "            )\n",
    "            # Shape: [batch_size, sequence_length, embedding_dimension_size]\n",
    "\n",
    "            # Pass through RNN layers\n",
    "            rnn_output_tensor, final_hidden_state = (\n",
    "                self.recurrent_neural_network_layers(\n",
    "                    character_embeddings, hidden_state_tensor\n",
    "                )\n",
    "            )\n",
    "            # rnn_output_tensor shape: [batch_size, sequence_length, hidden_state_dimension_size]\n",
    "\n",
    "            # Project to vocabulary size\n",
    "            output_logits = self.output_projection_layer(rnn_output_tensor)\n",
    "            # Shape: [batch_size, sequence_length, vocabulary_size_for_embeddings]\n",
    "\n",
    "            return output_logits, final_hidden_state\n",
    "\n",
    "        except RuntimeError as runtime_error:\n",
    "            logger.error(f\"Runtime error in forward pass: {runtime_error}\")\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "        except Exception as forward_error:\n",
    "            logger.error(f\"Unexpected error in forward pass: {forward_error}\")\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "\n",
    "    # rnn style\n",
    "    def initialize_hidden_state(\n",
    "        self, batch_size: int, device: torch.device\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Initialize hidden state tensor for the RNN.\n",
    "\n",
    "        Args:\n",
    "            batch_size (int): Size of the current batch\n",
    "            device (torch.device): Device to create tensor on\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Initialized hidden state tensor\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If batch_size is invalid\n",
    "            Exception: For any other errors\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if batch_size <= 0:\n",
    "                raise ValueError(\"batch_size must be positive\")\n",
    "\n",
    "            hidden_state_tensor = torch.zeros(\n",
    "                self.number_of_rnn_layers,\n",
    "                batch_size,\n",
    "                self.hidden_state_dimension_size,\n",
    "                device=device,\n",
    "            )\n",
    "\n",
    "            return hidden_state_tensor\n",
    "\n",
    "        except ValueError as value_error:\n",
    "            logger.error(f\"Value error initializing hidden state: {value_error}\")\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "        except Exception as hidden_error:\n",
    "            logger.error(f\"Error initializing hidden state: {hidden_error}\")\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "\n",
    "    # # LSTM style\n",
    "    # def initialize_hidden_state(self, batch_size: int, device: torch.device) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    #     \"\"\"\n",
    "    #     Initialize hidden state and cell state tensors for the LSTM.\n",
    "        \n",
    "    #     Args:\n",
    "    #         batch_size (int): Size of the current batch\n",
    "    #         device (torch.device): Device to create tensor on\n",
    "            \n",
    "    #     Returns:\n",
    "    #         Tuple[torch.Tensor, torch.Tensor]: Initialized (hidden_state, cell_state) tuple\n",
    "            \n",
    "    #     Raises:\n",
    "    #         ValueError: If batch_size is invalid\n",
    "    #         Exception: For any other errors\n",
    "    #     \"\"\"\n",
    "    #     try:\n",
    "    #         if batch_size <= 0:\n",
    "    #             raise ValueError(\"batch_size must be positive\")\n",
    "            \n",
    "    #         # LSTM needs both hidden state and cell state\n",
    "    #         hidden_state_tensor = torch.zeros(\n",
    "    #             self.number_of_rnn_layers,\n",
    "    #             batch_size,\n",
    "    #             self.hidden_state_dimension_size,\n",
    "    #             device=device\n",
    "    #         )\n",
    "            \n",
    "    #         cell_state_tensor = torch.zeros(\n",
    "    #             self.number_of_rnn_layers,\n",
    "    #             batch_size,\n",
    "    #             self.hidden_state_dimension_size,\n",
    "    #             device=device\n",
    "    #         )\n",
    "            \n",
    "    #         return (hidden_state_tensor, cell_state_tensor)  # Return tuple instead of single tensor\n",
    "            \n",
    "    #     except ValueError as value_error:\n",
    "    #         logger.error(f\"Value error initializing LSTM hidden state: {value_error}\")\n",
    "    #         traceback.print_exc()\n",
    "    #         raise\n",
    "    #     except Exception as hidden_error:\n",
    "    #         logger.error(f\"Error initializing LSTM hidden state: {hidden_error}\")\n",
    "    #         traceback.print_exc()\n",
    "    #         raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7f4086d-aba0-461b-a8b8-a5e8fbf0ed24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class GutenbergModelTrainer:\n",
    "    \"\"\"\n",
    "    Training manager for the Gutenberg RNN model.\n",
    "\n",
    "    This class handles the complete training process including model training,\n",
    "    validation, checkpointing, and progress monitoring.\n",
    "\n",
    "    Attributes:\n",
    "        model_to_train (GutenbergRNNModel): The RNN model to train\n",
    "        training_data_loader (DataLoader): DataLoader for training data\n",
    "        validation_data_loader (Optional[DataLoader]): DataLoader for validation data\n",
    "        loss_function_for_training (nn.Module): Loss function for training\n",
    "        optimizer_for_model_parameters (optim.Optimizer): Optimizer for model parameters\n",
    "        computation_device (torch.device): Device for computations\n",
    "        training_loss_history (List[float]): History of training losses\n",
    "        validation_loss_history (List[float]): History of validation losses\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_to_train: GutenbergRNNModel,\n",
    "        training_data_loader: DataLoader,\n",
    "        validation_data_loader: Optional[DataLoader] = None,\n",
    "        learning_rate_for_optimizer: float = 0.001,\n",
    "        device_name_string: str = \"auto\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the model trainer.\n",
    "\n",
    "        Args:\n",
    "            model_to_train (GutenbergRNNModel): Model to train\n",
    "            training_data_loader (DataLoader): Training data loader\n",
    "            validation_data_loader (Optional[DataLoader]): Validation data loader\n",
    "            learning_rate_for_optimizer (float): Learning rate for optimizer\n",
    "            device_name_string (str): Device name (\"auto\", \"cpu\", \"cuda\")\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If parameters are invalid\n",
    "            Exception: For any other initialization errors\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(\"Initializing Gutenberg model trainer\")\n",
    "\n",
    "            # Validate inputs\n",
    "            if learning_rate_for_optimizer <= 0:\n",
    "                raise ValueError(\"learning_rate_for_optimizer must be positive\")\n",
    "\n",
    "            # Set up computation device\n",
    "            self.computation_device = self._setup_computation_device(device_name_string)\n",
    "            logger.info(f\"Using device: {self.computation_device}\")\n",
    "\n",
    "            # Store components\n",
    "            self.model_to_train = model_to_train.to(self.computation_device)\n",
    "            self.training_data_loader = training_data_loader\n",
    "            self.validation_data_loader = validation_data_loader\n",
    "\n",
    "            # Initialize training components\n",
    "            self.loss_function_for_training = nn.CrossEntropyLoss()\n",
    "            self.optimizer_for_model_parameters = optim.Adam(\n",
    "                self.model_to_train.parameters(), lr=learning_rate_for_optimizer\n",
    "            )\n",
    "\n",
    "            # Initialize training history\n",
    "            self.training_loss_history = []\n",
    "            self.validation_loss_history = []\n",
    "\n",
    "            logger.info(\"Trainer initialized successfully\")\n",
    "\n",
    "        except ValueError as value_error:\n",
    "            logger.error(f\"Value error during trainer initialization: {value_error}\")\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "        except Exception as init_error:\n",
    "            logger.error(\n",
    "                f\"Unexpected error during trainer initialization: {init_error}\"\n",
    "            )\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "\n",
    "    def _setup_computation_device(self, device_name_string: str) -> torch.device:\n",
    "        \"\"\"\n",
    "        Set up the computation device for training.\n",
    "\n",
    "        Args:\n",
    "            device_name_string (str): Device specification\n",
    "\n",
    "        Returns:\n",
    "            torch.device: Configured device\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If device name is invalid\n",
    "            RuntimeError: If requested device is unavailable\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if device_name_string == \"auto\":\n",
    "                if torch.cuda.is_available():\n",
    "                    device = torch.device(\"cuda\")\n",
    "                    logger.info(\"CUDA available, using GPU\")\n",
    "                elif torch.backends.mps.is_available():\n",
    "                    device = torch.device(\"mps\")\n",
    "                    logger.info(\"MPS available, using Apple Silicon GPU\")\n",
    "                else:\n",
    "                    device = torch.device(\"cpu\")\n",
    "                    logger.info(\"Using CPU\")\n",
    "            elif device_name_string == \"cpu\":\n",
    "                device = torch.device(\"cpu\")\n",
    "            elif device_name_string == \"cuda\":\n",
    "                if not torch.cuda.is_available():\n",
    "                    raise RuntimeError(\"CUDA requested but not available\")\n",
    "                device = torch.device(\"cuda\")\n",
    "            elif device_name_string == \"mps\":\n",
    "                if not torch.backends.mps.is_available():\n",
    "                    raise RuntimeError(\"MPS requested but not available\")\n",
    "                device = torch.device(\"mps\")\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid device name: {device_name_string}\")\n",
    "\n",
    "            return device\n",
    "\n",
    "        except (ValueError, RuntimeError) as device_error:\n",
    "            logger.error(f\"Device setup error: {device_error}\")\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "        except Exception as setup_error:\n",
    "            logger.error(f\"Unexpected error setting up device: {setup_error}\")\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "\n",
    "    def train_model_for_epochs(\n",
    "        self,\n",
    "        number_of_training_epochs: int,\n",
    "        checkpoint_save_directory: Optional[str] = None,\n",
    "        checkpoint_save_frequency: int = 10,\n",
    "    ) -> Dict[str, List[float]]:\n",
    "        \"\"\"\n",
    "        Train the model for specified number of epochs.\n",
    "\n",
    "        Args:\n",
    "            number_of_training_epochs (int): Number of epochs to train\n",
    "            checkpoint_save_directory (Optional[str]): Directory to save checkpoints\n",
    "            checkpoint_save_frequency (int): Frequency of checkpoint saving\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, List[float]]: Training history with losses\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If parameters are invalid\n",
    "            Exception: For any training errors\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Starting training for {number_of_training_epochs} epochs\")\n",
    "\n",
    "            # Validate parameters\n",
    "            if number_of_training_epochs <= 0:\n",
    "                raise ValueError(\"number_of_training_epochs must be positive\")\n",
    "            if checkpoint_save_frequency <= 0:\n",
    "                raise ValueError(\"checkpoint_save_frequency must be positive\")\n",
    "\n",
    "            # Create checkpoint directory if specified\n",
    "            checkpoint_directory_path = None\n",
    "            if checkpoint_save_directory is not None:\n",
    "                checkpoint_directory_path = Path(checkpoint_save_directory)\n",
    "                checkpoint_directory_path.mkdir(parents=True, exist_ok=True)\n",
    "                logger.info(f\"Checkpoint directory: {checkpoint_directory_path}\")\n",
    "\n",
    "            # Training loop\n",
    "            for current_epoch in range(number_of_training_epochs):\n",
    "                try:\n",
    "                    logger.info(\n",
    "                        f\"Starting epoch {current_epoch + 1}/{number_of_training_epochs}\"\n",
    "                    )\n",
    "\n",
    "                    # Train for one epoch\n",
    "                    epoch_training_loss = self._train_single_epoch()\n",
    "                    self.training_loss_history.append(epoch_training_loss)\n",
    "\n",
    "                    # Validate if validation data is available\n",
    "                    if self.validation_data_loader is not None:\n",
    "                        epoch_validation_loss = self._validate_single_epoch()\n",
    "                        self.validation_loss_history.append(epoch_validation_loss)\n",
    "\n",
    "                        logger.info(\n",
    "                            f\"Epoch {current_epoch + 1}: \"\n",
    "                            f\"Train Loss = {epoch_training_loss:.4f}, \"\n",
    "                            f\"Val Loss = {epoch_validation_loss:.4f}\"\n",
    "                        )\n",
    "                    else:\n",
    "                        logger.info(\n",
    "                            f\"Epoch {current_epoch + 1}: Train Loss = {epoch_training_loss:.4f}\"\n",
    "                        )\n",
    "\n",
    "                    # Save checkpoint if needed\n",
    "                    if (\n",
    "                        checkpoint_directory_path is not None\n",
    "                        and (current_epoch + 1) % checkpoint_save_frequency == 0\n",
    "                    ):\n",
    "                        self._save_model_checkpoint(\n",
    "                            checkpoint_directory_path, current_epoch + 1\n",
    "                        )\n",
    "\n",
    "                except Exception as epoch_error:\n",
    "                    logger.error(f\"Error in epoch {current_epoch + 1}: {epoch_error}\")\n",
    "                    traceback.print_exc()\n",
    "                    continue\n",
    "\n",
    "            # Save final checkpoint\n",
    "            if checkpoint_directory_path is not None:\n",
    "                self._save_model_checkpoint(\n",
    "                    checkpoint_directory_path, number_of_training_epochs, is_final=True\n",
    "                )\n",
    "\n",
    "            training_history = {\n",
    "                \"training_loss\": self.training_loss_history,\n",
    "                \"validation_loss\": self.validation_loss_history,\n",
    "            }\n",
    "\n",
    "            logger.info(\"Training completed successfully\")\n",
    "            return training_history\n",
    "\n",
    "        except ValueError as value_error:\n",
    "            logger.error(f\"Value error during training: {value_error}\")\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "        except Exception as training_error:\n",
    "            logger.error(f\"Unexpected error during training: {training_error}\")\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "\n",
    "    def _train_single_epoch(self) -> float:\n",
    "        \"\"\"\n",
    "        Train the model for a single epoch.\n",
    "\n",
    "        Returns:\n",
    "            float: Average training loss for the epoch\n",
    "\n",
    "        Raises:\n",
    "            RuntimeError: If training fails\n",
    "            Exception: For any other errors\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.model_to_train.train()\n",
    "            total_loss_for_epoch = 0.0\n",
    "            total_batches_processed = 0\n",
    "\n",
    "            for batch_index, (input_sequences, target_characters) in enumerate(\n",
    "                self.training_data_loader\n",
    "            ):\n",
    "                try:\n",
    "                    # Move data to device\n",
    "                    input_sequences = input_sequences.to(self.computation_device)\n",
    "                    target_characters = target_characters.to(self.computation_device)\n",
    "\n",
    "                    # Reset gradients\n",
    "                    self.optimizer_for_model_parameters.zero_grad()\n",
    "\n",
    "                    # Forward pass\n",
    "                    output_logits, _ = self.model_to_train(input_sequences)\n",
    "\n",
    "                    # Calculate loss (use last time step for character prediction)\n",
    "                    last_timestep_logits = output_logits[:, -1, :]\n",
    "                    batch_loss = self.loss_function_for_training(\n",
    "                        last_timestep_logits, target_characters\n",
    "                    )\n",
    "\n",
    "                    # Backward pass\n",
    "                    batch_loss.backward()\n",
    "\n",
    "                    # Gradient clipping for stability\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                        self.model_to_train.parameters(), max_norm=1.0\n",
    "                    )\n",
    "\n",
    "                    # Update parameters\n",
    "                    self.optimizer_for_model_parameters.step()\n",
    "\n",
    "                    # Accumulate loss\n",
    "                    total_loss_for_epoch += batch_loss.item()\n",
    "                    total_batches_processed += 1\n",
    "\n",
    "                    # Log progress periodically\n",
    "                    if batch_index % 100 == 0:\n",
    "                        logger.debug(\n",
    "                            f\"Batch {batch_index}: Loss = {batch_loss.item():.4f}\"\n",
    "                        )\n",
    "\n",
    "                except Exception as batch_error:\n",
    "                    logger.error(f\"Error processing batch {batch_index}: {batch_error}\")\n",
    "                    continue\n",
    "\n",
    "            if total_batches_processed == 0:\n",
    "                raise RuntimeError(\"No batches were processed successfully\")\n",
    "\n",
    "            average_epoch_loss = total_loss_for_epoch / total_batches_processed\n",
    "            return average_epoch_loss\n",
    "\n",
    "        except RuntimeError as runtime_error:\n",
    "            logger.error(f\"Runtime error during single epoch training: {runtime_error}\")\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "        except Exception as epoch_error:\n",
    "            logger.error(\n",
    "                f\"Unexpected error during single epoch training: {epoch_error}\"\n",
    "            )\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "\n",
    "    def _validate_single_epoch(self) -> float:\n",
    "        \"\"\"\n",
    "        Validate the model for a single epoch.\n",
    "\n",
    "        Returns:\n",
    "            float: Average validation loss for the epoch\n",
    "\n",
    "        Raises:\n",
    "            RuntimeError: If validation fails\n",
    "            Exception: For any other errors\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.model_to_train.eval()\n",
    "            total_validation_loss = 0.0\n",
    "            total_validation_batches = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch_index, (input_sequences, target_characters) in enumerate(\n",
    "                    self.validation_data_loader\n",
    "                ):\n",
    "                    try:\n",
    "                        # Move data to device\n",
    "                        input_sequences = input_sequences.to(self.computation_device)\n",
    "                        target_characters = target_characters.to(\n",
    "                            self.computation_device\n",
    "                        )\n",
    "\n",
    "                        # Forward pass\n",
    "                        output_logits, _ = self.model_to_train(input_sequences)\n",
    "\n",
    "                        # Calculate loss\n",
    "                        last_timestep_logits = output_logits[:, -1, :]\n",
    "                        batch_validation_loss = self.loss_function_for_training(\n",
    "                            last_timestep_logits, target_characters\n",
    "                        )\n",
    "\n",
    "                        # Accumulate loss\n",
    "                        total_validation_loss += batch_validation_loss.item()\n",
    "                        total_validation_batches += 1\n",
    "\n",
    "                    except Exception as validation_batch_error:\n",
    "                        logger.error(\n",
    "                            f\"Error in validation batch {batch_index}: {validation_batch_error}\"\n",
    "                        )\n",
    "                        continue\n",
    "\n",
    "            if total_validation_batches == 0:\n",
    "                raise RuntimeError(\"No validation batches were processed successfully\")\n",
    "\n",
    "            average_validation_loss = total_validation_loss / total_validation_batches\n",
    "            return average_validation_loss\n",
    "\n",
    "        except RuntimeError as runtime_error:\n",
    "            logger.error(f\"Runtime error during validation: {runtime_error}\")\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "        except Exception as validation_error:\n",
    "            logger.error(f\"Unexpected error during validation: {validation_error}\")\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "\n",
    "    def _save_model_checkpoint(\n",
    "        self, checkpoint_directory_path: Path, epoch_number: int, is_final: bool = False\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Save model checkpoint to disk.\n",
    "\n",
    "        Args:\n",
    "            checkpoint_directory_path (Path): Directory to save checkpoint\n",
    "            epoch_number (int): Current epoch number\n",
    "            is_final (bool): Whether this is the final checkpoint\n",
    "\n",
    "        Raises:\n",
    "            Exception: If checkpoint saving fails\n",
    "        \"\"\"\n",
    "        try:\n",
    "            checkpoint_filename = f\"Gutenberg_rnn_epoch_{epoch_number}.pth\"\n",
    "            if is_final:\n",
    "                checkpoint_filename = \"Gutenberg_rnn_final.pth\"\n",
    "\n",
    "            checkpoint_file_path = checkpoint_directory_path / checkpoint_filename\n",
    "\n",
    "            checkpoint_data = {\n",
    "                \"epoch\": epoch_number,\n",
    "                \"model_state_dict\": self.model_to_train.state_dict(),\n",
    "                \"optimizer_state_dict\": self.optimizer_for_model_parameters.state_dict(),\n",
    "                \"training_loss_history\": self.training_loss_history,\n",
    "                \"validation_loss_history\": self.validation_loss_history,\n",
    "                \"model_config\": {\n",
    "                    \"vocabulary_size\": self.model_to_train.vocabulary_size_for_embeddings,\n",
    "                    \"embedding_dim\": self.model_to_train.embedding_dimension_size,\n",
    "                    \"hidden_dim\": self.model_to_train.hidden_state_dimension_size,\n",
    "                    \"num_layers\": self.model_to_train.number_of_rnn_layers,\n",
    "                },\n",
    "            }\n",
    "\n",
    "            torch.save(checkpoint_data, checkpoint_file_path)\n",
    "            logger.info(f\"Saved checkpoint: {checkpoint_file_path}\")\n",
    "\n",
    "        except Exception as checkpoint_error:\n",
    "            logger.error(f\"Error saving checkpoint: {checkpoint_error}\")\n",
    "            traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cba49f99-48b1-4794-9aa3-9c588556173b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class GutenbergTextGenerator:\n",
    "    \"\"\"\n",
    "    Text generator for the trained Gutenberg RNN model.\n",
    "\n",
    "    This class handles text generation using the trained RNN model,\n",
    "    including various sampling strategies and generation parameters.\n",
    "\n",
    "    Attributes:\n",
    "        trained_model (GutenbergRNNModel): The trained RNN model\n",
    "        text_dataset (GutenbergTextDataset): Dataset for character mappings\n",
    "        computation_device (torch.device): Device for computations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        trained_model: GutenbergRNNModel,\n",
    "        text_dataset: GutenbergTextDataset,\n",
    "        device_name_string: str = \"auto\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the text generator.\n",
    "\n",
    "        Args:\n",
    "            trained_model (GutenbergRNNModel): Trained model for generation\n",
    "            text_dataset (GutenbergTextDataset): Dataset with character mappings\n",
    "            device_name_string (str): Device name for computations\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If inputs are invalid\n",
    "            Exception: For any other initialization errors\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(\"Initializing Gutenberg text generator\")\n",
    "\n",
    "            # Set up device\n",
    "            self.computation_device = self._setup_computation_device(device_name_string)\n",
    "\n",
    "            # Store components\n",
    "            self.trained_model = trained_model.to(self.computation_device)\n",
    "            self.text_dataset = text_dataset\n",
    "\n",
    "            # Set model to evaluation mode\n",
    "            self.trained_model.eval()\n",
    "\n",
    "            logger.info(\"Text generator initialized successfully\")\n",
    "\n",
    "        except Exception as init_error:\n",
    "            logger.error(f\"Error initializing text generator: {init_error}\")\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "    \n",
    "    def save_generated_text_to_file(\n",
    "        self,\n",
    "        generated_text_content: str,\n",
    "        output_directory_path: str = \"generated_texts\",\n",
    "        filename_prefix: str = \"gutenberg_generated\",\n",
    "        include_generation_metadata: bool = True\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Save generated text to a timestamped file.\n",
    "        \n",
    "        Args:\n",
    "            generated_text_content (str): The generated text content to save\n",
    "            output_directory_path (str): Directory to save the generated text file\n",
    "            filename_prefix (str): Prefix for the generated filename\n",
    "            include_generation_metadata (bool): Whether to include metadata in the file\n",
    "            \n",
    "        Returns:\n",
    "            str: Full path of the saved file\n",
    "            \n",
    "        Raises:\n",
    "            OSError: If file creation or directory creation fails\n",
    "            Exception: For any other unexpected errors during file saving\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(\"Saving generated text to timestamped file\")\n",
    "            \n",
    "            # Create output directory if it doesn't exist\n",
    "            output_directory_path_object = Path(output_directory_path)\n",
    "            output_directory_path_object.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            # Generate timestamp for filename\n",
    "            current_timestamp = datetime.now()\n",
    "            timestamp_string = current_timestamp.strftime(\"%Y%m%d_%H%M%S\")\n",
    "            \n",
    "            # Create filename with timestamp\n",
    "            output_filename = f\"{filename_prefix}_{timestamp_string}.txt\"\n",
    "            full_output_file_path = output_directory_path_object / output_filename\n",
    "            \n",
    "            # Prepare content to save\n",
    "            content_to_save = \"\"\n",
    "            \n",
    "            if include_generation_metadata:\n",
    "                # Add metadata header\n",
    "                metadata_header = f\"\"\"# Gutenberg Text Generation Output\n",
    "# Generated on: {current_timestamp.strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "# Model: Gutenberg RNN/LSTM\n",
    "# Text Length: {len(generated_text_content)} characters\n",
    "# ============================================================\n",
    "\n",
    "\"\"\"\n",
    "                content_to_save += metadata_header\n",
    "            \n",
    "            # Add the generated text\n",
    "            content_to_save += generated_text_content\n",
    "            \n",
    "            # Write to file with error handling\n",
    "            try:\n",
    "                with open(full_output_file_path, 'w', encoding='utf-8') as output_file_handle:\n",
    "                    output_file_handle.write(content_to_save)\n",
    "                \n",
    "                logger.info(f\"Successfully saved generated text to: {full_output_file_path}\")\n",
    "                logger.info(f\"File size: {len(content_to_save)} characters\")\n",
    "                \n",
    "                return str(full_output_file_path)\n",
    "                \n",
    "            except UnicodeEncodeError as encoding_error:\n",
    "                logger.error(f\"Unicode encoding error while saving file: {encoding_error}\")\n",
    "                # Try with different encoding as fallback\n",
    "                try:\n",
    "                    with open(full_output_file_path, 'w', encoding='latin-1') as output_file_handle:\n",
    "                        output_file_handle.write(content_to_save)\n",
    "                    logger.info(f\"Saved with latin-1 encoding to: {full_output_file_path}\")\n",
    "                    return str(full_output_file_path)\n",
    "                except Exception as fallback_error:\n",
    "                    logger.error(f\"Fallback encoding also failed: {fallback_error}\")\n",
    "                    raise\n",
    "            \n",
    "        except OSError as file_system_error:\n",
    "            logger.error(f\"File system error while saving generated text: {file_system_error}\")\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "        except Exception as save_error:\n",
    "            logger.error(f\"Unexpected error while saving generated text: {save_error}\")\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "\n",
    "    def _setup_computation_device(self, device_name_string: str) -> torch.device:\n",
    "        \"\"\"\n",
    "        Set up computation device for generation.\n",
    "\n",
    "        Args:\n",
    "            device_name_string (str): Device specification\n",
    "\n",
    "        Returns:\n",
    "            torch.device: Configured device\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if device_name_string == \"auto\":\n",
    "                if torch.cuda.is_available():\n",
    "                    return torch.device(\"cuda\")\n",
    "                elif torch.backends.mps.is_available():\n",
    "                    return torch.device(\"mps\")\n",
    "                else:\n",
    "                    return torch.device(\"cpu\")\n",
    "            else:\n",
    "                return torch.device(device_name_string)\n",
    "        except Exception as device_error:\n",
    "            logger.error(f\"Error setting up device: {device_error}\")\n",
    "            return torch.device(\"cpu\")\n",
    "\n",
    "    def generate_text_with_seed(\n",
    "        self,\n",
    "        seed_text_string: str,\n",
    "        generation_length: int = 200,\n",
    "        temperature_for_sampling: float = 1.0,\n",
    "        use_top_k_sampling: bool = False,\n",
    "        top_k_value: int = 50,\n",
    "        save_to_file: bool = False,\n",
    "        output_directory: str = \"generated_texts\"\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Generate text using a seed string with optional file saving.\n",
    "        \n",
    "        Args:\n",
    "            seed_text_string (str): Initial text to start generation\n",
    "            generation_length (int): Number of characters to generate\n",
    "            temperature_for_sampling (float): Temperature for sampling (higher = more random)\n",
    "            use_top_k_sampling (bool): Whether to use top-k sampling\n",
    "            top_k_value (int): K value for top-k sampling\n",
    "            save_to_file (bool): Whether to save generated text to file\n",
    "            output_directory (str): Directory to save output file\n",
    "            \n",
    "        Returns:\n",
    "            str: Generated text including the seed\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If parameters are invalid\n",
    "            Exception: For any generation errors\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Generating text with seed: '{seed_text_string[:50]}...'\")\n",
    "\n",
    "            # Validate parameters\n",
    "            if generation_length <= 0:\n",
    "                raise ValueError(\"generation_length must be positive\")\n",
    "            if temperature_for_sampling <= 0:\n",
    "                raise ValueError(\"temperature_for_sampling must be positive\")\n",
    "            if use_top_k_sampling and top_k_value <= 0:\n",
    "                raise ValueError(\n",
    "                    \"top_k_value must be positive when using top-k sampling\"\n",
    "                )\n",
    "\n",
    "            # Convert seed text to indices\n",
    "            seed_indices = self._convert_text_to_indices(seed_text_string)\n",
    "            if len(seed_indices) == 0:\n",
    "                raise ValueError(\"Could not convert seed text to indices\")\n",
    "\n",
    "            generated_indices = seed_indices.copy()\n",
    "\n",
    "            # Initialize hidden state\n",
    "            hidden_state = self.trained_model.initialize_hidden_state(\n",
    "                1, self.computation_device\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Process seed text through the model\n",
    "                if len(seed_indices) > 1:\n",
    "                    seed_tensor = torch.tensor(\n",
    "                        [seed_indices[:-1]],\n",
    "                        dtype=torch.long,\n",
    "                        device=self.computation_device,\n",
    "                    )\n",
    "                    _, hidden_state = self.trained_model(seed_tensor, hidden_state)\n",
    "\n",
    "                # Generate new characters\n",
    "                current_input = torch.tensor(\n",
    "                    [[seed_indices[-1]]],\n",
    "                    dtype=torch.long,\n",
    "                    device=self.computation_device,\n",
    "                )\n",
    "\n",
    "                for generation_step in range(generation_length):\n",
    "                    try:\n",
    "                        # Get next character probabilities\n",
    "                        output_logits, hidden_state = self.trained_model(\n",
    "                            current_input, hidden_state\n",
    "                        )\n",
    "                        next_char_logits = (\n",
    "                            output_logits[0, -1, :] / temperature_for_sampling\n",
    "                        )\n",
    "\n",
    "                        # Apply sampling strategy\n",
    "                        if use_top_k_sampling:\n",
    "                            next_char_index = self._sample_top_k(\n",
    "                                next_char_logits, top_k_value\n",
    "                            )\n",
    "                        else:\n",
    "                            next_char_probabilities = F.softmax(next_char_logits, dim=0)\n",
    "                            next_char_index = torch.multinomial(\n",
    "                                next_char_probabilities, 1\n",
    "                            ).item()\n",
    "\n",
    "                        # Add to generated sequence\n",
    "                        generated_indices.append(next_char_index)\n",
    "\n",
    "                        # Update input for next iteration\n",
    "                        current_input = torch.tensor(\n",
    "                            [[next_char_index]],\n",
    "                            dtype=torch.long,\n",
    "                            device=self.computation_device,\n",
    "                        )\n",
    "\n",
    "                    except Exception as generation_step_error:\n",
    "                        logger.error(\n",
    "                            f\"Error in generation step {generation_step}: {generation_step_error}\"\n",
    "                        )\n",
    "                        break\n",
    "\n",
    "            # Convert indices back to text\n",
    "            generated_text = self.text_dataset.decode_integer_sequence_to_text(\n",
    "                generated_indices\n",
    "            )\n",
    "\n",
    "            # Save to file if requested\n",
    "            if save_to_file:\n",
    "                try:\n",
    "                    saved_file_path = self.save_generated_text_to_file(\n",
    "                        generated_text_content=generated_text,\n",
    "                        output_directory_path=output_directory\n",
    "                    )\n",
    "                    logger.info(f\"Generated text saved to: {saved_file_path}\")\n",
    "                except Exception as save_error:\n",
    "                    logger.warning(f\"Failed to save generated text to file: {save_error}\")\n",
    "                    # Don't raise - file saving failure shouldn't stop text generation\n",
    "\n",
    "            logger.info(f\"Generated {len(generated_text)} characters\")\n",
    "            return generated_text\n",
    "\n",
    "        except ValueError as value_error:\n",
    "            logger.error(f\"Value error during text generation: {value_error}\")\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "        except Exception as generation_error:\n",
    "            logger.error(f\"Unexpected error during text generation: {generation_error}\")\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "\n",
    "    def _convert_text_to_indices(self, text_string: str) -> List[int]:\n",
    "        \"\"\"\n",
    "        Convert text string to list of character indices.\n",
    "\n",
    "        Args:\n",
    "            text_string (str): Text to convert\n",
    "\n",
    "        Returns:\n",
    "            List[int]: List of character indices\n",
    "        \"\"\"\n",
    "        try:\n",
    "            indices_list = []\n",
    "            for character in text_string:\n",
    "                if character in self.text_dataset.character_to_index_mapping:\n",
    "                    character_index = self.text_dataset.character_to_index_mapping[\n",
    "                        character\n",
    "                    ]\n",
    "                    indices_list.append(character_index)\n",
    "                else:\n",
    "                    logger.warning(f\"Character not in vocabulary: {repr(character)}\")\n",
    "\n",
    "            return indices_list\n",
    "\n",
    "        except Exception as conversion_error:\n",
    "            logger.error(f\"Error converting text to indices: {conversion_error}\")\n",
    "            return []\n",
    "\n",
    "    def _sample_top_k(self, logits_tensor: torch.Tensor, k_value: int) -> int:\n",
    "        \"\"\"\n",
    "        Sample from top-k most likely tokens.\n",
    "\n",
    "        Args:\n",
    "            logits_tensor (torch.Tensor): Logits for sampling\n",
    "            k_value (int): Number of top tokens to consider\n",
    "\n",
    "        Returns:\n",
    "            int: Sampled token index\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Get top k values and indices\n",
    "            top_k_values, top_k_indices = torch.topk(logits_tensor, k_value)\n",
    "\n",
    "            # Convert to probabilities\n",
    "            top_k_probabilities = F.softmax(top_k_values, dim=0)\n",
    "\n",
    "            # Sample from top k\n",
    "            sampled_index_in_top_k = torch.multinomial(top_k_probabilities, 1).item()\n",
    "            sampled_token_index = top_k_indices[sampled_index_in_top_k].item()\n",
    "\n",
    "            return sampled_token_index\n",
    "\n",
    "        except Exception as sampling_error:\n",
    "            logger.error(f\"Error in top-k sampling: {sampling_error}\")\n",
    "            # Fallback to greedy sampling\n",
    "            return torch.argmax(logits_tensor).item()\n",
    "\n",
    "\n",
    "def create_sample_Gutenberg_text() -> str:\n",
    "    \"\"\"\n",
    "    Create a sample Gutenberg text for demonstration purposes.\n",
    "\n",
    "    Returns:\n",
    "        str: Sample Gutenberg text\n",
    "    \"\"\"\n",
    "    sample_text = \"\"\"\n",
    "    To be, or not to be, that is the question:\n",
    "    Whether 'tis nobler in the mind to suffer\n",
    "    The slings and arrows of outrageous fortune,\n",
    "    Or to take arms against a sea of troubles\n",
    "    And by opposing end them. To die—to sleep,\n",
    "    No more; and by a sleep to say we end\n",
    "    The heart-ache and the thousand natural shocks\n",
    "    That flesh is heir to: 'tis a consummation\n",
    "    Devoutly to be wish'd. To die, to sleep;\n",
    "    To sleep, perchance to dream—ay, there's the rub:\n",
    "    For in that sleep of death what dreams may come,\n",
    "    When we have shuffled off this mortal coil,\n",
    "    Must give us pause—there's the respect\n",
    "    That makes calamity of so long life.\n",
    "    \n",
    "    But soft you, the fair Ophelia!\n",
    "    Nymph, in thy orisons\n",
    "    Be all my sins remember'd.\n",
    "    \n",
    "    Lord Hamlet is a prince, out of thy star;\n",
    "    This must not be: and then I prescripts gave her,\n",
    "    That she should lock herself from his resort,\n",
    "    Admit no messengers, receive no tokens.\n",
    "    Which done, she took the fruits of my advice;\n",
    "    And he, repulsed—a short tale to make—\n",
    "    Fell into a sadness, then into a fast,\n",
    "    Thence to a watch, thence into a weakness,\n",
    "    Thence to a lightness, and, by this declension,\n",
    "    Into the madness wherein now he raves,\n",
    "    And all we mourn for.\n",
    "    \"\"\"\n",
    "    return sample_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b845f50f-2b57-4a0f-962b-766c26222b32",
   "metadata": {},
   "source": [
    "# Gutenberg Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56ba534c-9a80-4e80-8ac6-5ccee120cdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# Gutenberg.org Download\n",
    "#########################\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import traceback\n",
    "import requests\n",
    "import urllib.parse\n",
    "from pathlib import Path\n",
    "from typing import Union, List, Optional, Dict, Tuple\n",
    "import re\n",
    "import hashlib\n",
    "\n",
    "# Configure logging for the module\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def get_gutenberg_books_outputs_corpus_file_path_with_individual_saves(\n",
    "    book_urls: Union[str, List[str]],\n",
    "    remove_text: bool = False,\n",
    "    parent_file_directory: Optional[str] = None,\n",
    "    corpus_filename: str = \"corpus.txt\",\n",
    "    save_individual_books: bool = False,\n",
    "    individual_books_subdirectory_name: str = \"individual_books\",\n",
    "    individual_filename_prefix: str = \"book_\",\n",
    "    individual_filename_suffix: str = \".txt\",\n",
    "    use_url_based_individual_filenames: bool = True,\n",
    "    remove_individual_files_after_corpus_creation: bool = False\n",
    ") -> Dict[str, Union[str, List[str], None]]:\n",
    "    \"\"\"\n",
    "    Download Project Gutenberg book(s) and save as a corpus file for RNN training.\n",
    "    Optionally saves individual books as separate files before combining into corpus.\n",
    "\n",
    "    This function downloads books from Project Gutenberg URLs, optionally saves each \n",
    "    book as an individual file, then combines all content into a single corpus file \n",
    "    suitable for machine learning training purposes.\n",
    "\n",
    "    Args:\n",
    "        book_urls (Union[str, List[str]]): Single URL string or list of URLs to Gutenberg books\n",
    "        remove_text (bool): Whether to remove the corpus file after processing (default: False)\n",
    "        parent_file_directory (Optional[str]): Directory to save files (default: current directory)\n",
    "        corpus_filename (str): Name for the output corpus file (default: \"corpus.txt\")\n",
    "        save_individual_books (bool): Whether to save each book as individual file (default: False)\n",
    "        individual_books_subdirectory_name (str): Name of subdirectory for individual books (default: \"individual_books\")\n",
    "        individual_filename_prefix (str): Prefix for individual book filenames (default: \"book_\")\n",
    "        individual_filename_suffix (str): File extension for individual books (default: \".txt\")\n",
    "        use_url_based_individual_filenames (bool): Use URL-derived names vs sequential numbering (default: True)\n",
    "        remove_individual_files_after_corpus_creation (bool): Remove individual files after corpus is created (default: False)\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Union[str, List[str], None]]: Dictionary containing:\n",
    "            - 'corpus_file_path': Path to the corpus file (or None if removed successfully)\n",
    "            - 'individual_file_paths': List of individual file paths (empty list if not saved or removed)\n",
    "            - 'download_success_count': Number of successfully downloaded books\n",
    "            - 'download_failure_count': Number of failed downloads\n",
    "            - 'total_character_count': Total characters in combined corpus\n",
    "            - 'processing_status': Status message about the operation\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If book_urls is empty, invalid, or contains non-string values\n",
    "        TypeError: If book_urls is not string or list type\n",
    "        requests.exceptions.RequestException: If HTTP request fails\n",
    "        OSError: If file system operations fail\n",
    "        Exception: If any other download, processing, or save operation fails\n",
    "\n",
    "    Examples:\n",
    "        # Basic usage - just corpus file\n",
    "        result = get_gutenberg_books_outputs_corpus_file_path_with_individual_saves(\n",
    "            \"https://www.gutenberg.org/files/74/74-0.txt\"\n",
    "        )\n",
    "        \n",
    "        # Save individual books and corpus\n",
    "        result = get_gutenberg_books_outputs_corpus_file_path_with_individual_saves(\n",
    "            [\"https://www.gutenberg.org/files/74/74-0.txt\", \n",
    "             \"https://www.gutenberg.org/files/1342/1342-0.txt\"],\n",
    "            save_individual_books=True,\n",
    "            parent_file_directory=\"/path/to/output\"\n",
    "        )\n",
    "        \n",
    "        # Advanced usage with custom naming\n",
    "        result = get_gutenberg_books_outputs_corpus_file_path_with_individual_saves(\n",
    "            book_urls_list,\n",
    "            save_individual_books=True,\n",
    "            individual_filename_prefix=\"gutenberg_book_\",\n",
    "            use_url_based_individual_filenames=False,\n",
    "            remove_individual_files_after_corpus_creation=True\n",
    "        )\n",
    "    \"\"\"\n",
    "    # Initialize return structure with safe defaults\n",
    "    operation_result_dictionary = {\n",
    "        'corpus_file_path': None,\n",
    "        'individual_file_paths': [],\n",
    "        'download_success_count': 0,\n",
    "        'download_failure_count': 0,\n",
    "        'total_character_count': 0,\n",
    "        'processing_status': 'initialized'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        logger.info(\"Starting enhanced Gutenberg download pipeline with optional individual file saves\")\n",
    "        \n",
    "        # Input validation and normalization\n",
    "        if book_urls is None:\n",
    "            raise ValueError(\"book_urls parameter cannot be None\")\n",
    "            \n",
    "        if isinstance(book_urls, str):\n",
    "            if not book_urls.strip():\n",
    "                raise ValueError(\"book_urls string cannot be empty or whitespace only\")\n",
    "            normalized_url_list = [book_urls.strip()]\n",
    "        elif isinstance(book_urls, list):\n",
    "            if not book_urls:\n",
    "                raise ValueError(\"book_urls list cannot be empty\")\n",
    "            # Validate all items are strings\n",
    "            normalized_url_list = []\n",
    "            for index, url_item in enumerate(book_urls):\n",
    "                if not isinstance(url_item, str):\n",
    "                    raise TypeError(f\"All URLs must be strings. Item at index {index} is type {type(url_item)}\")\n",
    "                if not url_item.strip():\n",
    "                    logger.warning(f\"Skipping empty URL at index {index}\")\n",
    "                    continue\n",
    "                normalized_url_list.append(url_item.strip())\n",
    "            \n",
    "            if not normalized_url_list:\n",
    "                raise ValueError(\"No valid non-empty URLs found in provided list\")\n",
    "        else:\n",
    "            raise TypeError(f\"book_urls must be string or list, got {type(book_urls)}\")\n",
    "            \n",
    "        logger.info(f\"Processing {len(normalized_url_list)} URL(s)\")\n",
    "        \n",
    "        # Setup directory structure\n",
    "        try:\n",
    "            if parent_file_directory:\n",
    "                parent_directory_path = Path(parent_file_directory)\n",
    "                parent_directory_path.mkdir(parents=True, exist_ok=True)\n",
    "                logger.info(f\"Created/verified parent directory: {parent_directory_path}\")\n",
    "                \n",
    "                if save_individual_books:\n",
    "                    individual_books_directory_path = parent_directory_path / individual_books_subdirectory_name\n",
    "                    individual_books_directory_path.mkdir(parents=True, exist_ok=True)\n",
    "                    logger.info(f\"Created/verified individual books directory: {individual_books_directory_path}\")\n",
    "                else:\n",
    "                    individual_books_directory_path = None\n",
    "            else:\n",
    "                parent_directory_path = Path.cwd()\n",
    "                if save_individual_books:\n",
    "                    individual_books_directory_path = Path(individual_books_subdirectory_name)\n",
    "                    individual_books_directory_path.mkdir(parents=True, exist_ok=True)\n",
    "                    logger.info(f\"Created/verified individual books directory: {individual_books_directory_path}\")\n",
    "                else:\n",
    "                    individual_books_directory_path = None\n",
    "                    \n",
    "        except OSError as directory_creation_error:\n",
    "            logger.error(f\"Failed to create directory structure: {directory_creation_error}\")\n",
    "            operation_result_dictionary['processing_status'] = f'directory_creation_failed: {str(directory_creation_error)}'\n",
    "            raise\n",
    "        \n",
    "        # Download all books and optionally save individually\n",
    "        all_downloaded_text_content_list = []\n",
    "        individual_file_paths_list = []\n",
    "        successful_download_counter = 0\n",
    "        failed_download_counter = 0\n",
    "        \n",
    "        for url_index, current_book_url in enumerate(normalized_url_list):\n",
    "            try:\n",
    "                logger.info(f\"Downloading book {url_index + 1}/{len(normalized_url_list)}: {current_book_url}\")\n",
    "                \n",
    "                # Validate URL format\n",
    "                parsed_url_result = urllib.parse.urlparse(current_book_url)\n",
    "                if not parsed_url_result.scheme or not parsed_url_result.netloc:\n",
    "                    raise ValueError(f\"Invalid URL format: {current_book_url}\")\n",
    "                \n",
    "                # Download the book with comprehensive error handling\n",
    "                try:\n",
    "                    http_response = requests.get(\n",
    "                        current_book_url, \n",
    "                        timeout=30,\n",
    "                        headers={'User-Agent': 'Python-Gutenberg-Downloader/1.0'}\n",
    "                    )\n",
    "                    http_response.raise_for_status()\n",
    "                    \n",
    "                except requests.exceptions.Timeout as timeout_error:\n",
    "                    logger.error(f\"Timeout downloading {current_book_url}: {timeout_error}\")\n",
    "                    failed_download_counter += 1\n",
    "                    if len(normalized_url_list) == 1:\n",
    "                        operation_result_dictionary['processing_status'] = f'single_url_timeout: {str(timeout_error)}'\n",
    "                        raise\n",
    "                    continue\n",
    "                    \n",
    "                except requests.exceptions.ConnectionError as connection_error:\n",
    "                    logger.error(f\"Connection error downloading {current_book_url}: {connection_error}\")\n",
    "                    failed_download_counter += 1\n",
    "                    if len(normalized_url_list) == 1:\n",
    "                        operation_result_dictionary['processing_status'] = f'single_url_connection_error: {str(connection_error)}'\n",
    "                        raise\n",
    "                    continue\n",
    "                    \n",
    "                except requests.exceptions.HTTPError as http_error:\n",
    "                    logger.error(f\"HTTP error downloading {current_book_url}: {http_error}\")\n",
    "                    failed_download_counter += 1\n",
    "                    if len(normalized_url_list) == 1:\n",
    "                        operation_result_dictionary['processing_status'] = f'single_url_http_error: {str(http_error)}'\n",
    "                        raise\n",
    "                    continue\n",
    "                \n",
    "                # Decode the content with encoding fallback\n",
    "                try:\n",
    "                    current_book_text_content = http_response.text\n",
    "                except UnicodeDecodeError as decode_error:\n",
    "                    logger.warning(f\"Unicode decode error for {current_book_url}, trying latin-1: {decode_error}\")\n",
    "                    current_book_text_content = http_response.content.decode('latin-1')\n",
    "                \n",
    "                if not current_book_text_content.strip():\n",
    "                    logger.warning(f\"Downloaded content from {current_book_url} is empty or whitespace only\")\n",
    "                    failed_download_counter += 1\n",
    "                    if len(normalized_url_list) == 1:\n",
    "                        operation_result_dictionary['processing_status'] = 'single_url_empty_content'\n",
    "                        raise ValueError(\"Downloaded content is empty\")\n",
    "                    continue\n",
    "                \n",
    "                all_downloaded_text_content_list.append(current_book_text_content)\n",
    "                successful_download_counter += 1\n",
    "                \n",
    "                logger.info(f\"Successfully downloaded {len(current_book_text_content)} characters from {current_book_url}\")\n",
    "                \n",
    "                # Optionally save individual book file\n",
    "                if save_individual_books and individual_books_directory_path:\n",
    "                    try:\n",
    "                        # Generate individual filename\n",
    "                        if use_url_based_individual_filenames:\n",
    "                            # Create filename from URL\n",
    "                            url_hash = hashlib.md5(current_book_url.encode('utf-8')).hexdigest()[:8]\n",
    "                            url_filename_part = re.sub(r'[^\\w\\-_\\.]', '_', parsed_url_result.path.split('/')[-1])\n",
    "                            if not url_filename_part or url_filename_part == '_':\n",
    "                                url_filename_part = f\"url_{url_hash}\"\n",
    "                            individual_book_filename = f\"{individual_filename_prefix}{url_filename_part}_{url_hash}{individual_filename_suffix}\"\n",
    "                        else:\n",
    "                            # Use sequential numbering\n",
    "                            individual_book_filename = f\"{individual_filename_prefix}{url_index + 1:03d}{individual_filename_suffix}\"\n",
    "                        \n",
    "                        individual_book_file_path = individual_books_directory_path / individual_book_filename\n",
    "                        \n",
    "                        # Save individual book file\n",
    "                        with open(individual_book_file_path, 'w', encoding='utf-8') as individual_book_file_handle:\n",
    "                            individual_book_file_handle.write(current_book_text_content)\n",
    "                        \n",
    "                        individual_file_paths_list.append(str(individual_book_file_path))\n",
    "                        logger.info(f\"Saved individual book to: {individual_book_file_path}\")\n",
    "                        \n",
    "                    except OSError as individual_file_save_error:\n",
    "                        logger.error(f\"Failed to save individual book {url_index + 1}: {individual_file_save_error}\")\n",
    "                        # Continue processing even if individual save fails\n",
    "                        \n",
    "            except Exception as book_download_error:\n",
    "                logger.error(f\"Failed to process book from {current_book_url}: {book_download_error}\")\n",
    "                traceback.print_exc()\n",
    "                failed_download_counter += 1\n",
    "                if len(normalized_url_list) == 1:\n",
    "                    operation_result_dictionary['processing_status'] = f'single_url_processing_error: {str(book_download_error)}'\n",
    "                    raise\n",
    "                continue\n",
    "        \n",
    "        # Validate that we have content to work with\n",
    "        if not all_downloaded_text_content_list:\n",
    "            error_message = f\"No books downloaded successfully. Failed: {failed_download_counter}, Total attempted: {len(normalized_url_list)}\"\n",
    "            logger.error(error_message)\n",
    "            operation_result_dictionary['processing_status'] = 'no_successful_downloads'\n",
    "            raise Exception(error_message)\n",
    "        \n",
    "        logger.info(f\"Download summary: {successful_download_counter} successful, {failed_download_counter} failed\")\n",
    "        \n",
    "        # Combine all books with clear separators\n",
    "        try:\n",
    "            books_separator_text = \"\\n\\n===== NEW BOOK =====\\n\\n\"\n",
    "            combined_corpus_text = books_separator_text.join(all_downloaded_text_content_list)\n",
    "            total_character_count = len(combined_corpus_text)\n",
    "            \n",
    "            logger.info(f\"Combined corpus contains {total_character_count} characters from {len(all_downloaded_text_content_list)} books\")\n",
    "            \n",
    "        except Exception as text_combination_error:\n",
    "            logger.error(f\"Failed to combine downloaded texts: {text_combination_error}\")\n",
    "            operation_result_dictionary['processing_status'] = f'text_combination_error: {str(text_combination_error)}'\n",
    "            raise\n",
    "        \n",
    "        # Determine corpus file path\n",
    "        try:\n",
    "            if parent_file_directory:\n",
    "                corpus_file_full_path = os.path.join(parent_file_directory, corpus_filename)\n",
    "            else:\n",
    "                corpus_file_full_path = corpus_filename\n",
    "                \n",
    "            # Validate corpus filename\n",
    "            if not corpus_filename or not corpus_filename.strip():\n",
    "                raise ValueError(\"corpus_filename cannot be empty or whitespace only\")\n",
    "                \n",
    "        except Exception as path_determination_error:\n",
    "            logger.error(f\"Failed to determine corpus file path: {path_determination_error}\")\n",
    "            operation_result_dictionary['processing_status'] = f'path_determination_error: {str(path_determination_error)}'\n",
    "            raise\n",
    "            \n",
    "        # Save corpus file\n",
    "        try:\n",
    "            with open(corpus_file_full_path, 'w', encoding='utf-8') as corpus_file_handle:\n",
    "                corpus_file_handle.write(combined_corpus_text)\n",
    "                \n",
    "            logger.info(f\"Successfully saved corpus file to: {corpus_file_full_path}\")\n",
    "            \n",
    "        except OSError as corpus_file_save_error:\n",
    "            logger.error(f\"Failed to save corpus file: {corpus_file_save_error}\")\n",
    "            operation_result_dictionary['processing_status'] = f'corpus_file_save_error: {str(corpus_file_save_error)}'\n",
    "            raise\n",
    "        \n",
    "        # Optionally remove individual files after corpus creation\n",
    "        if save_individual_books and remove_individual_files_after_corpus_creation and individual_file_paths_list:\n",
    "            try:\n",
    "                removed_individual_files_count = 0\n",
    "                individual_file_removal_errors = []\n",
    "                \n",
    "                for individual_file_path in individual_file_paths_list[:]:  # Create copy for safe iteration\n",
    "                    try:\n",
    "                        os.remove(individual_file_path)\n",
    "                        individual_file_paths_list.remove(individual_file_path)\n",
    "                        removed_individual_files_count += 1\n",
    "                        logger.info(f\"Removed individual file: {individual_file_path}\")\n",
    "                        \n",
    "                    except OSError as individual_removal_error:\n",
    "                        error_message = f\"Could not remove individual file {individual_file_path}: {individual_removal_error}\"\n",
    "                        logger.warning(error_message)\n",
    "                        individual_file_removal_errors.append(error_message)\n",
    "                \n",
    "                logger.info(f\"Removed {removed_individual_files_count} individual files\")\n",
    "                if individual_file_removal_errors:\n",
    "                    logger.warning(f\"Individual file removal errors: {individual_file_removal_errors}\")\n",
    "                    \n",
    "            except Exception as individual_files_cleanup_error:\n",
    "                logger.warning(f\"Error during individual files cleanup: {individual_files_cleanup_error}\")\n",
    "        \n",
    "        # Optionally remove corpus file if requested\n",
    "        corpus_file_path_for_return = corpus_file_full_path\n",
    "        if remove_text:\n",
    "            try:\n",
    "                os.remove(corpus_file_full_path)\n",
    "                logger.info(f\"Removed corpus file as requested: {corpus_file_full_path}\")\n",
    "                corpus_file_path_for_return = None\n",
    "                \n",
    "            except OSError as corpus_removal_error:\n",
    "                logger.warning(f\"Could not remove corpus file: {corpus_removal_error}\")\n",
    "                # Return path anyway since file still exists\n",
    "        \n",
    "        # Populate successful result dictionary\n",
    "        operation_result_dictionary.update({\n",
    "            'corpus_file_path': corpus_file_path_for_return,\n",
    "            'individual_file_paths': individual_file_paths_list,\n",
    "            'download_success_count': successful_download_counter,\n",
    "            'download_failure_count': failed_download_counter,\n",
    "            'total_character_count': total_character_count,\n",
    "            'processing_status': 'completed_successfully'\n",
    "        })\n",
    "        \n",
    "        logger.info(\"Enhanced Gutenberg download pipeline completed successfully\")\n",
    "        return operation_result_dictionary\n",
    "        \n",
    "    except Exception as pipeline_error:\n",
    "        logger.error(f\"Enhanced pipeline error: {pipeline_error}\")\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        # Update result dictionary with error information\n",
    "        operation_result_dictionary['processing_status'] = f'pipeline_error: {str(pipeline_error)}'\n",
    "        \n",
    "        # For production safety, still return the dictionary even on error\n",
    "        # This allows calling code to check the status and handle appropriately\n",
    "        raise pipeline_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a568716-b4ca-4dd8-a7b6-91384e8390db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_Gutenberg_rnn_pipeline(\n",
    "    corpus_file_path,\n",
    "    path_for_project_files,\n",
    "):\n",
    "    \"\"\"\n",
    "    Demonstrate the complete Gutenberg RNN pipeline.\n",
    "\n",
    "    This function shows how to:\n",
    "    1. Create sample data\n",
    "    2. Initialize dataset and model\n",
    "    3. Train the model\n",
    "    4. Generate text\n",
    "\n",
    "    Raises:\n",
    "        Exception: If any step of the demonstration fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(\"Starting Gutenberg RNN demonstration pipeline\")\n",
    "\n",
    "        # # for dev/testing\n",
    "        # # Create sample text file\n",
    "        # sample_text_file_path = \"sample_Gutenberg.txt\"\n",
    "        # sample_text_content = create_sample_Gutenberg_text()\n",
    "\n",
    "        # try:\n",
    "        #     with open(sample_text_file_path, \"w\", encoding=\"utf-8\") as sample_file:\n",
    "        #         sample_file.write(sample_text_content)\n",
    "        #     logger.info(f\"Created sample text file: {sample_text_file_path}\")\n",
    "        # except Exception as file_creation_error:\n",
    "        #     logger.error(f\"Error creating sample file: {file_creation_error}\")\n",
    "        #     raise\n",
    "\n",
    "        # Initialize dataset\n",
    "        try:\n",
    "            sequence_length = 50\n",
    "            Gutenberg_dataset = GutenbergTextDataset(\n",
    "                text_file_path=corpus_file_path,\n",
    "                sequence_length_for_training=sequence_length,\n",
    "            )\n",
    "            logger.info(f\"Dataset created with {len(Gutenberg_dataset)} samples\")\n",
    "        except Exception as dataset_error:\n",
    "            logger.error(f\"Error creating dataset: {dataset_error}\")\n",
    "            raise\n",
    "\n",
    "        # Create data loaders\n",
    "        try:\n",
    "            batch_size = 32\n",
    "            training_data_loader = DataLoader(\n",
    "                Gutenberg_dataset, batch_size=batch_size, shuffle=True, drop_last=True\n",
    "            )\n",
    "            logger.info(f\"Data loader created with batch size {batch_size}\")\n",
    "        except Exception as dataloader_error:\n",
    "            logger.error(f\"Error creating data loader: {dataloader_error}\")\n",
    "            raise\n",
    "\n",
    "        # Initialize model\n",
    "        try:\n",
    "            vocabulary_size = Gutenberg_dataset.get_vocabulary_size()\n",
    "            rnn_model = GutenbergRNNModel(\n",
    "                vocabulary_size_for_embeddings=vocabulary_size,\n",
    "                embedding_dimension_size=64,\n",
    "                hidden_state_dimension_size=128,\n",
    "                number_of_rnn_layers=2,\n",
    "            )\n",
    "            logger.info(f\"Model created with vocabulary size {vocabulary_size}\")\n",
    "        except Exception as model_error:\n",
    "            logger.error(f\"Error creating model: {model_error}\")\n",
    "            raise\n",
    "\n",
    "        # Initialize trainer\n",
    "        try:\n",
    "            model_trainer = GutenbergModelTrainer(\n",
    "                model_to_train=rnn_model,\n",
    "                training_data_loader=training_data_loader,\n",
    "                learning_rate_for_optimizer=0.001,\n",
    "            )\n",
    "            logger.info(\"Trainer initialized\")\n",
    "        except Exception as trainer_error:\n",
    "            logger.error(f\"Error creating trainer: {trainer_error}\")\n",
    "            raise\n",
    "\n",
    "        # Train model\n",
    "        try:\n",
    "            training_epochs = 5  # Small number for demonstration\n",
    "            training_history = model_trainer.train_model_for_epochs(\n",
    "                number_of_training_epochs=training_epochs,\n",
    "                checkpoint_save_directory=\"checkpoints\",\n",
    "            )\n",
    "            logger.info(\"Model training completed\")\n",
    "        except Exception as training_error:\n",
    "            logger.error(f\"Error during training: {training_error}\")\n",
    "            raise\n",
    "\n",
    "        # Create text generator\n",
    "        try:\n",
    "            text_generator = GutenbergTextGenerator(\n",
    "                trained_model=rnn_model, text_dataset=Gutenberg_dataset\n",
    "            )\n",
    "            logger.info(\"Text generator created\")\n",
    "        except Exception as generator_error:\n",
    "            logger.error(f\"Error creating text generator: {generator_error}\")\n",
    "            raise\n",
    "\n",
    "        # Generate sample text with file saving\n",
    "        try:\n",
    "            seed_text = \"To be or not to be\"\n",
    "            generated_text = text_generator.generate_text_with_seed(\n",
    "                seed_text_string=seed_text,\n",
    "                generation_length=100,\n",
    "                temperature_for_sampling=0.8,\n",
    "                save_to_file=True,  # geerated text as file (saving)\n",
    "                output_directory=\"generated_gutenberg_texts\"  # Custom directory\n",
    "            )\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"GENERATED GUTENBERG TEXT:\")\n",
    "            print(\"=\"*80)\n",
    "            print(generated_text)\n",
    "            print(\"=\"*80)\n",
    "            \n",
    "            logger.info(\"Text generation and file saving completed successfully\")\n",
    "        except Exception as generation_error:\n",
    "            logger.error(f\"Error during text generation: {generation_error}\")\n",
    "            raise\n",
    "\n",
    "        # Plot training history\n",
    "        try:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(training_history[\"training_loss\"], label=\"Training Loss\")\n",
    "            plt.title(\"Gutenberg RNN Training Loss\")\n",
    "            plt.xlabel(\"Epoch\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.savefig(f\"{path_for_project_files}/training_loss.png\")\n",
    "            plt.show()\n",
    "            logger.info(\"Training loss plot created\")\n",
    "        except Exception as plotting_error:\n",
    "            logger.error(f\"Error creating plot: {plotting_error}\")\n",
    "            # Don't raise - plotting is not critical\n",
    "\n",
    "        # # for dev/testing\n",
    "        # # Clean up sample file\n",
    "        # try:\n",
    "        #     os.remove(sample_text_file_path)\n",
    "        #     logger.info(\"Cleaned up sample text file\")\n",
    "        # except Exception as cleanup_error:\n",
    "        #     logger.warning(f\"Could not clean up sample file: {cleanup_error}\")\n",
    "\n",
    "        logger.info(\"Gutenberg RNN demonstration completed successfully\")\n",
    "\n",
    "    except Exception as pipeline_error:\n",
    "        logger.error(f\"Error in demonstration pipeline: {pipeline_error}\")\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6c2fbdf-019c-4563-ab9c-4200ff06a511",
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "# Run Books\n",
    "############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eae810b2-645e-49b0-8ab8-1f6c17cdf21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 14:55:53,755 - __main__ - INFO - Starting enhanced Gutenberg download pipeline with optional individual file saves\n",
      "2025-10-10 14:55:53,755 - __main__ - INFO - Processing 3 URL(s)\n",
      "2025-10-10 14:55:53,756 - __main__ - INFO - Created/verified parent directory: project_files_directory\n",
      "2025-10-10 14:55:53,756 - __main__ - INFO - Created/verified individual books directory: project_files_directory/individual_books\n",
      "2025-10-10 14:55:53,756 - __main__ - INFO - Downloading book 1/3: https://www.gutenberg.org/ebooks/1513.txt.utf-8\n",
      "2025-10-10 14:55:54,230 - __main__ - INFO - Successfully downloaded 167429 characters from https://www.gutenberg.org/ebooks/1513.txt.utf-8\n",
      "2025-10-10 14:55:54,231 - __main__ - INFO - Saved individual book to: project_files_directory/individual_books/guten_book_1513.txt.utf-8_002e006e.txt\n",
      "2025-10-10 14:55:54,231 - __main__ - INFO - Downloading book 2/3: https://www.gutenberg.org/ebooks/1514.txt.utf-8\n",
      "2025-10-10 14:55:54,557 - __main__ - INFO - Successfully downloaded 119968 characters from https://www.gutenberg.org/ebooks/1514.txt.utf-8\n",
      "2025-10-10 14:55:54,558 - __main__ - INFO - Saved individual book to: project_files_directory/individual_books/guten_book_1514.txt.utf-8_896b6b5f.txt\n",
      "2025-10-10 14:55:54,558 - __main__ - INFO - Downloading book 3/3: https://www.gutenberg.org/ebooks/1515.txt.utf-8\n",
      "2025-10-10 14:55:54,926 - __main__ - INFO - Successfully downloaded 145209 characters from https://www.gutenberg.org/ebooks/1515.txt.utf-8\n",
      "2025-10-10 14:55:54,927 - __main__ - INFO - Saved individual book to: project_files_directory/individual_books/guten_book_1515.txt.utf-8_6a6c9dc1.txt\n",
      "2025-10-10 14:55:54,928 - __main__ - INFO - Download summary: 3 successful, 0 failed\n",
      "2025-10-10 14:55:54,928 - __main__ - INFO - Combined corpus contains 432654 characters from 3 books\n",
      "2025-10-10 14:55:54,929 - __main__ - INFO - Successfully saved corpus file to: project_files_directory/corpus.txt\n",
      "2025-10-10 14:55:54,929 - __main__ - INFO - Enhanced Gutenberg download pipeline completed successfully\n",
      "2025-10-10 14:55:54,930 - __main__ - INFO - Starting Gutenberg RNN main execution\n",
      "2025-10-10 14:55:54,930 - __main__ - INFO - Starting Gutenberg RNN demonstration pipeline\n",
      "2025-10-10 14:55:54,930 - __main__ - INFO - Initializing Gutenberg dataset from: project_files_directory/corpus.txt\n",
      "2025-10-10 14:55:54,931 - __main__ - INFO - Attempting to read file with encoding: utf-8\n",
      "2025-10-10 14:55:54,931 - __main__ - INFO - Successfully read file with encoding: utf-8\n",
      "2025-10-10 14:55:54,934 - __main__ - INFO - Created character mappings for 96 unique characters\n",
      "2025-10-10 14:55:54,947 - __main__ - INFO - Encoded text to 418579 integers\n",
      "2025-10-10 14:55:54,948 - __main__ - INFO - Dataset initialized successfully. Text length: 418579, Unique characters: 96\n",
      "2025-10-10 14:55:54,948 - __main__ - INFO - Dataset created with 418529 samples\n",
      "2025-10-10 14:55:54,949 - __main__ - INFO - Data loader created with batch size 32\n",
      "2025-10-10 14:55:54,949 - __main__ - INFO - Initializing Gutenberg RNN model\n",
      "2025-10-10 14:55:54,950 - __main__ - INFO - Model layers initialized successfully\n",
      "2025-10-10 14:55:54,950 - __main__ - INFO - Model initialized - Vocab: 96, Embed: 64, Hidden: 128, Layers: 2\n",
      "2025-10-10 14:55:54,950 - __main__ - INFO - Model created with vocabulary size 96\n",
      "2025-10-10 14:55:54,950 - __main__ - INFO - Initializing Gutenberg model trainer\n",
      "2025-10-10 14:55:54,972 - __main__ - INFO - CUDA available, using GPU\n",
      "2025-10-10 14:55:54,973 - __main__ - INFO - Using device: cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 3 books\n",
      "Individual files: ['project_files_directory/individual_books/guten_book_1513.txt.utf-8_002e006e.txt', 'project_files_directory/individual_books/guten_book_1514.txt.utf-8_896b6b5f.txt', 'project_files_directory/individual_books/guten_book_1515.txt.utf-8_6a6c9dc1.txt']\n",
      "Corpus file path: project_files_directory/corpus.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 14:55:55,551 - __main__ - INFO - Trainer initialized successfully\n",
      "2025-10-10 14:55:55,551 - __main__ - INFO - Trainer initialized\n",
      "2025-10-10 14:55:55,551 - __main__ - INFO - Starting training for 5 epochs\n",
      "2025-10-10 14:55:55,551 - __main__ - INFO - Checkpoint directory: checkpoints\n",
      "2025-10-10 14:55:55,552 - __main__ - INFO - Starting epoch 1/5\n",
      "2025-10-10 14:56:13,778 - __main__ - INFO - Epoch 1: Train Loss = 1.9996\n",
      "2025-10-10 14:56:13,778 - __main__ - INFO - Starting epoch 2/5\n",
      "2025-10-10 14:56:31,196 - __main__ - INFO - Epoch 2: Train Loss = 1.7763\n",
      "2025-10-10 14:56:31,197 - __main__ - INFO - Starting epoch 3/5\n",
      "2025-10-10 14:56:48,820 - __main__ - INFO - Epoch 3: Train Loss = 1.7301\n",
      "2025-10-10 14:56:48,820 - __main__ - INFO - Starting epoch 4/5\n",
      "2025-10-10 14:57:05,795 - __main__ - INFO - Epoch 4: Train Loss = 1.7092\n",
      "2025-10-10 14:57:05,795 - __main__ - INFO - Starting epoch 5/5\n",
      "2025-10-10 14:57:23,857 - __main__ - INFO - Epoch 5: Train Loss = 1.6957\n",
      "2025-10-10 14:57:23,860 - __main__ - INFO - Saved checkpoint: checkpoints/Gutenberg_rnn_final.pth\n",
      "2025-10-10 14:57:23,861 - __main__ - INFO - Training completed successfully\n",
      "2025-10-10 14:57:23,861 - __main__ - INFO - Model training completed\n",
      "2025-10-10 14:57:23,861 - __main__ - INFO - Initializing Gutenberg text generator\n",
      "2025-10-10 14:57:23,862 - __main__ - INFO - Text generator initialized successfully\n",
      "2025-10-10 14:57:23,863 - __main__ - INFO - Text generator created\n",
      "2025-10-10 14:57:23,863 - __main__ - INFO - Generating text with seed: 'To be or not to be...'\n",
      "2025-10-10 14:57:23,949 - __main__ - INFO - Saving generated text to timestamped file\n",
      "2025-10-10 14:57:23,950 - __main__ - INFO - Successfully saved generated text to: generated_gutenberg_texts/gutenberg_generated_20251010_145723.txt\n",
      "2025-10-10 14:57:23,951 - __main__ - INFO - File size: 311 characters\n",
      "2025-10-10 14:57:23,951 - __main__ - INFO - Generated text saved to: generated_gutenberg_texts/gutenberg_generated_20251010_145723.txt\n",
      "2025-10-10 14:57:23,951 - __main__ - INFO - Generated 118 characters\n",
      "2025-10-10 14:57:23,952 - __main__ - INFO - Text generation and file saving completed successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "GENERATED GUTENBERG TEXT:\n",
      "================================================================================\n",
      "To be or not to be Project Gutenberg thou of heaver, complowning it, and any with upon a goldon,\n",
      "And have sweet in hea\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 14:57:24,070 - __main__ - INFO - Training loss plot created\n",
      "2025-10-10 14:57:24,071 - __main__ - INFO - Gutenberg RNN demonstration completed successfully\n",
      "2025-10-10 14:57:24,072 - __main__ - INFO - Main execution completed successfully\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    path_for_project_files = \"project_files_directory\"\n",
    "\n",
    "    # Multiple books with concatenation\n",
    "    Gutenberg_urls = [\n",
    "        \"https://www.gutenberg.org/ebooks/1513.txt.utf-8\",  # Romeo and Juliet\n",
    "        \"https://www.gutenberg.org/ebooks/1514.txt.utf-8\",  # Hamlet\n",
    "        \"https://www.gutenberg.org/ebooks/1515.txt.utf-8\",  # Macbeth\n",
    "    ]\n",
    "    \n",
    "    '''\n",
    "    def get_gutenberg_books_outputs_corpus_file_path_with_individual_saves(\n",
    "        book_urls: Union[str, List[str]],\n",
    "        remove_text: bool = False,\n",
    "        parent_file_directory: Optional[str] = None,\n",
    "        corpus_filename: str = \"corpus.txt\",\n",
    "        save_individual_books: bool = False,\n",
    "        individual_books_subdirectory_name: str = \"individual_books\",\n",
    "        individual_filename_prefix: str = \"book_\",\n",
    "        individual_filename_suffix: str = \".txt\",\n",
    "        use_url_based_individual_filenames: bool = True,\n",
    "        remove_individual_files_after_corpus_creation: bool = False\n",
    "    ) -> Dict[str, Union[str, List[str], None]]:\n",
    "    \"\"\"\n",
    "    '''\n",
    "\n",
    "    result = get_gutenberg_books_outputs_corpus_file_path_with_individual_saves(\n",
    "        book_urls=Gutenberg_urls,\n",
    "        remove_text=False,\n",
    "        parent_file_directory=path_for_project_files,\n",
    "        corpus_filename=\"corpus.txt\",\n",
    "        save_individual_books=True,\n",
    "        individual_books_subdirectory_name=\"individual_books\",\n",
    "        individual_filename_prefix=\"guten_book_\",\n",
    "        individual_filename_suffix=\".txt\",\n",
    "        use_url_based_individual_filenames=True,\n",
    "        remove_individual_files_after_corpus_creation=False,\n",
    "    )\n",
    "\n",
    "    # Access detailed results\n",
    "    print(f\"Downloaded {result['download_success_count']} books\")\n",
    "    print(f\"Individual files: {result['individual_file_paths']}\")\n",
    "    print(f\"Corpus file path: {result['corpus_file_path']}\")\n",
    "\n",
    "    corpus_file_path = result['corpus_file_path']\n",
    "\n",
    "    \"\"\"\n",
    "    Main execution block for the Gutenberg RNN demonstration.\n",
    "\n",
    "    This block runs the complete demonstration pipeline when the script\n",
    "    is executed directly.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(\"Starting Gutenberg RNN main execution\")\n",
    "        run_Gutenberg_rnn_pipeline(\n",
    "            corpus_file_path,\n",
    "            path_for_project_files,\n",
    "        )\n",
    "        logger.info(\"Main execution completed successfully\")\n",
    "    except Exception as main_error:\n",
    "        logger.error(f\"Error in main execution: {main_error}\")\n",
    "        traceback.print_exc()\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fa3765-da8f-47f4-84c1-54485996df33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
