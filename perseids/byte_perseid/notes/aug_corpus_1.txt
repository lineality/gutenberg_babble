(env) oops@oops-Precision-7780:~/code/gutenberg_babble/perseids/byte_perseid$ python3 aug_nlp.py 

Input a path to a .txt file:
//data/alice.txt
test print alice
Directory 'data/augmented_texts/alice_20250928_172652/' ensured to exist.
Loading input file: data/alice.txt
Configuration: {'1_sentence_spacing': 'random', '2_newline_spacing': 'random', '3_punctuation_spacing': 'random', '4_double_newlines': 'random', '5_quote_style': 'random', '6_tabs_spaces': 'all', '7_double_spaces': 'skip', '8_brackets': 'random', '9_contractions': 'random', '10_capitalization': 'skip', '11_numbers': 'random', '12_synonyms': 'random'}
Randomization level: 0.7
Generating 3 augmented documents...
Generating document 1/3
Saved: data/augmented_texts/alice_20250928_172652/alice_aug_1_20250928_172653.txt
Generating document 2/3
Saved: data/augmented_texts/alice_20250928_172652/alice_aug_2_20250928_172654.txt
Generating document 3/3
Saved: data/augmented_texts/alice_20250928_172652/alice_aug_3_20250928_172654.txt
Successfully generated 3 augmented documents

Generated files:
  - data/augmented_texts/alice_20250928_172652/alice_aug_1_20250928_172653.txt
  - data/augmented_texts/alice_20250928_172652/alice_aug_2_20250928_172654.txt
  - data/augmented_texts/alice_20250928_172652/alice_aug_3_20250928_172654.txt
Processed: alice_aug_1_20250928_172653.txt
Processed: alice_aug_2_20250928_172654.txt
Processed: alice_aug_3_20250928_172654.txt

Success! Combined 3 files into data/augmented_texts/alice_20250928_172652/corpus_alice_20250928_172652.txt
Making corpus file...data/augmented_texts/alice_20250928_172652/corpus_alice_20250928_172652.txt
Magic 8-Ball, corpus creation succeded?? ...True!
(env) oops@oops-Precision-7780:~/code/gutenberg_babble/perseids/byte_perseid$ python3 train_on_docs_perseid_byte.py 

============================================================
Testing ByteTokenizer Integration
============================================================

Initializing ByteTokenizer...
  ‚úì Vocabulary size: 259
  ‚úì Special tokens: PAD=256, EOS=257, MASKUNK=258

Tokenizer test:
  Original: 'Hello, World! üåç'
  Tokens: [72, 101, 108, 108, 111, 44, 32, 87, 111, 114, 108, 100, 33, 32, 240, 159, 140, 141, 257] (19 tokens)
  Decoded: 'Hello, World! üåç<eos>'
  Match: True
  ‚úì Detected ByteTokenizer configuration (vocab_size=259)

Perseid-256M (balanced strategy):
  Target: 256M parameters
  Actual: 56.82M parameters
  Difference: -199.18M (-77.8%)
  Configuration:
    - emb_dim: 576
    - hidden_dim: 1536
    - n_layers: 16
    - n_heads: 3
    - head_dim: 192

Model config test:
  Vocab size: 259
  Expected: 259
  Match: True
  Model size: 56.82M parameters
Arguments passed to the script:
	Argument 0: train_on_docs_perseid_byte.py
Usage: python script.py <path>
Loading existing data from data/alice.txt

Enter a file path to a .txt file or for a demo say 'demo'
data/tripplealice.txt

============================================================
Perseid Document Training Pipeline
============================================================
Experiment: perseid_288m_20250928_173235
Output directory: ./models/perseid_288m_tripplealice/

========================================
Step 1: Loading Document
========================================

Loading document: data/tripplealice.txt
File size: 1.46 MB
Successfully loaded with utf-8 encoding
Document length: 1,532,612 characters

========================================
Step 2: Setting Up Model
========================================

Initializing ByteTokenizer...
  ‚úì Vocabulary size: 259
  ‚úì Special tokens: PAD=256, EOS=257, MASKUNK=258

============================================================
Initializing New Model
============================================================
Creating Perseid-288M (balanced strategy)
  ‚úì Detected ByteTokenizer configuration (vocab_size=259)

Perseid-288M (balanced strategy):
  Target: 288M parameters
  Actual: 71.65M parameters
  Difference: -216.35M (-75.1%)
  Configuration:
    - emb_dim: 640
    - hidden_dim: 1792
    - n_layers: 16
    - n_heads: 4
    - head_dim: 160
  ‚úì Detected ByteTokenizer configuration (vocab_size=259)

Initializing model with random weights...
  ‚úì Model initialized with random weights

Model Statistics:
  Total parameters: 71,812,480
  Trainable parameters: 71,812,480
  Model size (bfloat16): 0.144 GB
  Device: cuda

========================================
Step 3: Preparing Data
========================================

Creating data loaders with 90% train / 10% validation split
Train text: 1,379,350 chars
Val text: 153,262 chars
Tokenizing text of length 1,379,350 characters...
Created 3,000 training windows
Total tokens: 1,380,219
Window size: 512, Stride: 460
Effective overlap: 10.2%
Tokenizing text of length 153,262 characters...
Created 300 training windows
Total tokens: 153,614
Window size: 512, Stride: 512
Effective overlap: 0.0%

Train batches: 1,500
Val batches: 150

========================================
Step 4: Training Model
========================================

============================================================
Starting Training
============================================================

Starting from epoch 1, step 0
Total training steps: 2,625
Warmup steps: 100
Effective batch size: 8

========================================
Epoch 1/7
========================================
Step    50 | Train Loss: 3.7370 | Val Loss: 2.4719 | LR: 2.50e-04 | Tokens: 204,800
  ‚Üí Saved best model (val_loss: 2.4719)
Step   100 | Train Loss: 3.0564 | Val Loss: 2.1359 | LR: 5.00e-04 | Tokens: 409,600
  ‚Üí Saved best model (val_loss: 2.1359)
Step   150 | Train Loss: 2.7260 | Val Loss: 1.9414 | LR: 5.00e-04 | Tokens: 614,400
  ‚Üí Saved best model (val_loss: 1.9414)
Step   200 | Train Loss: 2.5172 | Val Loss: 1.8734 | LR: 4.98e-04 | Tokens: 819,200
  ‚Üí Saved best model (val_loss: 1.8734)
Step   250 | Train Loss: 2.3779 | Val Loss: 1.8180 | LR: 4.96e-04 | Tokens: 1,024,000
  ‚Üí Saved best model (val_loss: 1.8180)
Step   300 | Train Loss: 2.2717 | Val Loss: 1.7453 | LR: 4.92e-04 | Tokens: 1,228,800
  ‚Üí Saved best model (val_loss: 1.7453)
Step   350 | Train Loss: 2.1907 | Val Loss: 1.7055 | LR: 4.88e-04 | Tokens: 1,433,600
  ‚Üí Saved best model (val_loss: 1.7055)

Epoch 1 Summary:
  Average Train Loss: 2.1565
  Validation Loss: 1.7061
  Perplexity: 5.51

========================================
Epoch 2/7
========================================
Step   400 | Train Loss: 1.6330 | Val Loss: 1.6883 | LR: 4.83e-04 | Tokens: 1,638,400
  ‚Üí Saved best model (val_loss: 1.6883)
Step   450 | Train Loss: 1.6176 | Val Loss: 1.6367 | LR: 4.77e-04 | Tokens: 1,843,200
  ‚Üí Saved best model (val_loss: 1.6367)
Step   500 | Train Loss: 1.6070 | Val Loss: 1.6586 | LR: 4.70e-04 | Tokens: 2,048,000
Step   550 | Train Loss: 1.5924 | Val Loss: 1.6219 | LR: 4.62e-04 | Tokens: 2,252,800
  ‚Üí Saved best model (val_loss: 1.6219)
Step   600 | Train Loss: 1.5781 | Val Loss: 1.6055 | LR: 4.53e-04 | Tokens: 2,457,600
  ‚Üí Saved best model (val_loss: 1.6055)
Step   650 | Train Loss: 1.5635 | Val Loss: 1.6000 | LR: 4.44e-04 | Tokens: 2,662,400
  ‚Üí Saved best model (val_loss: 1.6000)
Step   700 | Train Loss: 1.5507 | Val Loss: 1.5578 | LR: 4.34e-04 | Tokens: 2,867,200
  ‚Üí Saved best model (val_loss: 1.5578)
Step   750 | Train Loss: 1.5393 | Val Loss: 1.5344 | LR: 4.23e-04 | Tokens: 3,072,000
  ‚Üí Saved best model (val_loss: 1.5344)

Epoch 2 Summary:
  Average Train Loss: 1.5393
  Validation Loss: 1.4893
  Perplexity: 4.43

========================================
Epoch 3/7
========================================
Step   800 | Train Loss: 1.3996 | Val Loss: 1.5203 | LR: 4.11e-04 | Tokens: 3,276,800
  ‚Üí Saved best model (val_loss: 1.5203)
Step   850 | Train Loss: 1.3936 | Val Loss: 1.4977 | LR: 3.99e-04 | Tokens: 3,481,600
  ‚Üí Saved best model (val_loss: 1.4977)
Step   900 | Train Loss: 1.3813 | Val Loss: 1.5117 | LR: 3.86e-04 | Tokens: 3,686,400
Step   950 | Train Loss: 1.3739 | Val Loss: 1.4633 | LR: 3.73e-04 | Tokens: 3,891,200
  ‚Üí Saved best model (val_loss: 1.4633)
Step  1000 | Train Loss: 1.3669 | Val Loss: 1.4648 | LR: 3.59e-04 | Tokens: 4,096,000
Step  1050 | Train Loss: 1.3575 | Val Loss: 1.4438 | LR: 3.45e-04 | Tokens: 4,300,800
  ‚Üí Saved best model (val_loss: 1.4438)
Step  1100 | Train Loss: 1.3469 | Val Loss: 1.4281 | LR: 3.30e-04 | Tokens: 4,505,600
  ‚Üí Saved best model (val_loss: 1.4281)

Epoch 3 Summary:
  Average Train Loss: 1.3426
  Validation Loss: 1.3180
  Perplexity: 3.74

========================================
Epoch 4/7
========================================
Step  1150 | Train Loss: 1.2163 | Val Loss: 1.3836 | LR: 3.15e-04 | Tokens: 4,710,400
  ‚Üí Saved best model (val_loss: 1.3836)
Step  1200 | Train Loss: 1.2037 | Val Loss: 1.3844 | LR: 3.00e-04 | Tokens: 4,915,200
Step  1250 | Train Loss: 1.1957 | Val Loss: 1.3719 | LR: 2.85e-04 | Tokens: 5,120,000
  ‚Üí Saved best model (val_loss: 1.3719)
Step  1300 | Train Loss: 1.1863 | Val Loss: 1.3578 | LR: 2.69e-04 | Tokens: 5,324,800
  ‚Üí Saved best model (val_loss: 1.3578)
Step  1350 | Train Loss: 1.1772 | Val Loss: 1.3375 | LR: 2.54e-04 | Tokens: 5,529,600
  ‚Üí Saved best model (val_loss: 1.3375)
Step  1400 | Train Loss: 1.1674 | Val Loss: 1.3125 | LR: 2.38e-04 | Tokens: 5,734,400
  ‚Üí Saved best model (val_loss: 1.3125)
Step  1450 | Train Loss: 1.1569 | Val Loss: 1.2688 | LR: 2.23e-04 | Tokens: 5,939,200
  ‚Üí Saved best model (val_loss: 1.2688)
Step  1500 | Train Loss: 1.1488 | Val Loss: 1.2477 | LR: 2.07e-04 | Tokens: 6,144,000
  ‚Üí Saved best model (val_loss: 1.2477)

Epoch 4 Summary:
  Average Train Loss: 1.1488
  Validation Loss: 1.1355
  Perplexity: 3.11

========================================
Epoch 5/7
========================================
Step  1550 | Train Loss: 1.0173 | Val Loss: 1.2219 | LR: 1.92e-04 | Tokens: 6,348,800
  ‚Üí Saved best model (val_loss: 1.2219)
Step  1600 | Train Loss: 1.0075 | Val Loss: 1.1953 | LR: 1.77e-04 | Tokens: 6,553,600
  ‚Üí Saved best model (val_loss: 1.1953)
Step  1650 | Train Loss: 0.9935 | Val Loss: 1.1844 | LR: 1.62e-04 | Tokens: 6,758,400
  ‚Üí Saved best model (val_loss: 1.1844)
Step  1700 | Train Loss: 0.9835 | Val Loss: 1.1477 | LR: 1.48e-04 | Tokens: 6,963,200
  ‚Üí Saved best model (val_loss: 1.1477)
Step  1750 | Train Loss: 0.9767 | Val Loss: 1.1414 | LR: 1.34e-04 | Tokens: 7,168,000
  ‚Üí Saved best model (val_loss: 1.1414)
Step  1800 | Train Loss: 0.9692 | Val Loss: 1.1301 | LR: 1.21e-04 | Tokens: 7,372,800
  ‚Üí Saved best model (val_loss: 1.1301)
Step  1850 | Train Loss: 0.9585 | Val Loss: 1.1223 | LR: 1.07e-04 | Tokens: 7,577,600
  ‚Üí Saved best model (val_loss: 1.1223)

Epoch 5 Summary:
  Average Train Loss: 0.9550
  Validation Loss: 0.9923
  Perplexity: 2.70

========================================
Epoch 6/7
========================================
Step  1900 | Train Loss: 0.8425 | Val Loss: 1.0957 | LR: 9.50e-05 | Tokens: 7,782,400
  ‚Üí Saved best model (val_loss: 1.0957)
Step  1950 | Train Loss: 0.8450 | Val Loss: 1.0832 | LR: 8.31e-05 | Tokens: 7,987,200
  ‚Üí Saved best model (val_loss: 1.0832)
Step  2000 | Train Loss: 0.8371 | Val Loss: 1.0809 | LR: 7.19e-05 | Tokens: 8,192,000
  ‚Üí Saved best model (val_loss: 1.0809)
Step  2050 | Train Loss: 0.8337 | Val Loss: 1.0699 | LR: 6.13e-05 | Tokens: 8,396,800
  ‚Üí Saved best model (val_loss: 1.0699)
Step  2100 | Train Loss: 0.8303 | Val Loss: 1.0656 | LR: 5.15e-05 | Tokens: 8,601,600
  ‚Üí Saved best model (val_loss: 1.0656)
Step  2150 | Train Loss: 0.8284 | Val Loss: 1.0625 | LR: 4.24e-05 | Tokens: 8,806,400
  ‚Üí Saved best model (val_loss: 1.0625)
Step  2200 | Train Loss: 0.8246 | Val Loss: 1.0621 | LR: 3.41e-05 | Tokens: 9,011,200
  ‚Üí Saved best model (val_loss: 1.0621)
Step  2250 | Train Loss: 0.8239 | Val Loss: 1.0570 | LR: 2.67e-05 | Tokens: 9,216,000
  ‚Üí Saved best model (val_loss: 1.0570)

Epoch 6 Summary:
  Average Train Loss: 0.8239
  Validation Loss: 0.9485
  Perplexity: 2.58

========================================
Epoch 7/7
========================================
Step  2300 | Train Loss: 0.7875 | Val Loss: 1.0574 | LR: 2.02e-05 | Tokens: 9,420,800
Step  2350 | Train Loss: 0.7900 | Val Loss: 1.0543 | LR: 1.45e-05 | Tokens: 9,625,600
  ‚Üí Saved best model (val_loss: 1.0543)
Step  2400 | Train Loss: 0.7933 | Val Loss: 1.0559 | LR: 9.73e-06 | Tokens: 9,830,400
Step  2450 | Train Loss: 0.7904 | Val Loss: 1.0559 | LR: 5.90e-06 | Tokens: 10,035,200
Step  2500 | Train Loss: 0.7891 | Val Loss: 1.0559 | LR: 3.02e-06 | Tokens: 10,240,000
Step  2550 | Train Loss: 0.7882 | Val Loss: 1.0555 | LR: 1.09e-06 | Tokens: 10,444,800
Step  2600 | Train Loss: 0.7889 | Val Loss: 1.0559 | LR: 1.21e-07 | Tokens: 10,649,600

Epoch 7 Summary:
  Average Train Loss: 0.7890
  Validation Loss: 0.9468
  Perplexity: 2.58

============================================================
Training Complete!
============================================================
Best validation loss: 1.0543
Total tokens seen: 10,752,000

========================================
Step 5: Saving Results
========================================

Saving training results to models/perseid_288m_tripplealice
  ‚úì Model weights saved to models/perseid_288m_tripplealice/perseid_model_final.pth
  ‚úì Configuration saved to models/perseid_288m_tripplealice/model_config.json
  ‚úì Training history saved to models/perseid_288m_tripplealice/training_history.json
  ‚úì Training curves saved to models/perseid_288m_tripplealice/training_curves.png
  ‚úì Training config saved to models/perseid_288m_tripplealice/training_config.json

All outputs saved to: models/perseid_288m_tripplealice

============================================================
Training Pipeline Complete!

========================================
Step 5.5: Sample Generation
========================================
Prompt: 'Once upon a time'
Output: Once upon a time to the prophecy are the poet and the last
by the 

Prompt: 'The meaning of life is'
Output: The meaning of life is the poem and prophesies the student of the
sense.

Prompt: 'In the beginning'
Output: In the beginning and the poem. The sound is not an inferior is son

============================================================
Model and results saved to: models/perseid_288m_tripplealice
(env) oops@oops-Precision-7780:~/code/gutenberg_babble/perseids/byte_perseid$ python3 train_on_docs_perseid_byte.py 

============================================================
Testing ByteTokenizer Integration
============================================================

Initializing ByteTokenizer...
  ‚úì Vocabulary size: 259
  ‚úì Special tokens: PAD=256, EOS=257, MASKUNK=258

Tokenizer test:
  Original: 'Hello, World! üåç'
  Tokens: [72, 101, 108, 108, 111, 44, 32, 87, 111, 114, 108, 100, 33, 32, 240, 159, 140, 141, 257] (19 tokens)
  Decoded: 'Hello, World! üåç<eos>'
  Match: True
  ‚úì Detected ByteTokenizer configuration (vocab_size=259)

Perseid-256M (balanced strategy):
  Target: 256M parameters
  Actual: 56.82M parameters
  Difference: -199.18M (-77.8%)
  Configuration:
    - emb_dim: 576
    - hidden_dim: 1536
    - n_layers: 16
    - n_heads: 3
    - head_dim: 192

Model config test:
  Vocab size: 259
  Expected: 259
  Match: True
  Model size: 56.82M parameters
Arguments passed to the script:
	Argument 0: train_on_docs_perseid_byte.py
Usage: python script.py <path>
Loading existing data from data/alice.txt

Enter a file path to a .txt file or for a demo say 'demo'
/home/oops/code/gutenberg_babble/perseids/byte_perseid/data/augmented_texts/alice_20250928_172652/corpus_alice_20250928_172652.txt

============================================================
Perseid Document Training Pipeline
============================================================
Experiment: perseid_288m_20250928_174950
Output directory: ./models/perseid_288m_corpus_alice_20250928_172652/

========================================
Step 1: Loading Document
========================================

Loading document: /home/oops/code/gutenberg_babble/perseids/byte_perseid/data/augmented_texts/alice_20250928_172652/corpus_alice_20250928_172652.txt
File size: 1.68 MB
Successfully loaded with utf-8 encoding
Document length: 1,759,965 characters

========================================
Step 2: Setting Up Model
========================================

Initializing ByteTokenizer...
  ‚úì Vocabulary size: 259
  ‚úì Special tokens: PAD=256, EOS=257, MASKUNK=258

============================================================
Initializing New Model
============================================================
Creating Perseid-288M (balanced strategy)
  ‚úì Detected ByteTokenizer configuration (vocab_size=259)

Perseid-288M (balanced strategy):
  Target: 288M parameters
  Actual: 71.65M parameters
  Difference: -216.35M (-75.1%)
  Configuration:
    - emb_dim: 640
    - hidden_dim: 1792
    - n_layers: 16
    - n_heads: 4
    - head_dim: 160
  ‚úì Detected ByteTokenizer configuration (vocab_size=259)

Initializing model with random weights...
  ‚úì Model initialized with random weights

Model Statistics:
  Total parameters: 71,812,480
  Trainable parameters: 71,812,480
  Model size (bfloat16): 0.144 GB
  Device: cuda

========================================
Step 3: Preparing Data
========================================

Creating data loaders with 90% train / 10% validation split
Train text: 1,583,968 chars
Val text: 175,997 chars
Tokenizing text of length 1,583,968 characters...
Created 3,445 training windows
Total tokens: 1,584,837
Window size: 512, Stride: 460
Effective overlap: 10.2%
Tokenizing text of length 175,997 characters...
Created 344 training windows
Total tokens: 176,349
Window size: 512, Stride: 512
Effective overlap: 0.0%

Train batches: 1,722
Val batches: 172

========================================
Step 4: Training Model
========================================

============================================================
Starting Training
============================================================

Starting from epoch 1, step 0
Total training steps: 3,010
Warmup steps: 100
Effective batch size: 8

========================================
Epoch 1/7
========================================
Step    50 | Train Loss: 3.6028 | Val Loss: 2.3500 | LR: 2.50e-04 | Tokens: 204,800
  ‚Üí Saved best model (val_loss: 2.3500)
Step   100 | Train Loss: 2.9202 | Val Loss: 2.0156 | LR: 5.00e-04 | Tokens: 409,600
  ‚Üí Saved best model (val_loss: 2.0156)
Step   150 | Train Loss: 2.6035 | Val Loss: 1.8555 | LR: 5.00e-04 | Tokens: 614,400
  ‚Üí Saved best model (val_loss: 1.8555)
Step   200 | Train Loss: 2.4114 | Val Loss: 1.7891 | LR: 4.99e-04 | Tokens: 819,200
  ‚Üí Saved best model (val_loss: 1.7891)
Step   250 | Train Loss: 2.2813 | Val Loss: 1.7383 | LR: 4.97e-04 | Tokens: 1,024,000
  ‚Üí Saved best model (val_loss: 1.7383)
Step   300 | Train Loss: 2.1857 | Val Loss: 1.7188 | LR: 4.94e-04 | Tokens: 1,228,800
  ‚Üí Saved best model (val_loss: 1.7188)
Step   350 | Train Loss: 2.1136 | Val Loss: 1.6812 | LR: 4.91e-04 | Tokens: 1,433,600
  ‚Üí Saved best model (val_loss: 1.6812)
Step   400 | Train Loss: 2.0548 | Val Loss: 1.6758 | LR: 4.87e-04 | Tokens: 1,638,400
  ‚Üí Saved best model (val_loss: 1.6758)

Epoch 1 Summary:
  Average Train Loss: 2.0229
  Validation Loss: 1.6283
  Perplexity: 5.10

========================================
Epoch 2/7
========================================
Step   450 | Train Loss: 1.5784 | Val Loss: 1.6695 | LR: 4.82e-04 | Tokens: 1,843,200
  ‚Üí Saved best model (val_loss: 1.6695)
Step   500 | Train Loss: 1.5591 | Val Loss: 1.6203 | LR: 4.77e-04 | Tokens: 2,048,000
  ‚Üí Saved best model (val_loss: 1.6203)
Step   550 | Train Loss: 1.5489 | Val Loss: 1.6195 | LR: 4.71e-04 | Tokens: 2,252,800
  ‚Üí Saved best model (val_loss: 1.6195)
Step   600 | Train Loss: 1.5404 | Val Loss: 1.6055 | LR: 4.64e-04 | Tokens: 2,457,600
  ‚Üí Saved best model (val_loss: 1.6055)
Step   650 | Train Loss: 1.5314 | Val Loss: 1.5836 | LR: 4.57e-04 | Tokens: 2,662,400
  ‚Üí Saved best model (val_loss: 1.5836)
Step   700 | Train Loss: 1.5241 | Val Loss: 1.5656 | LR: 4.49e-04 | Tokens: 2,867,200
  ‚Üí Saved best model (val_loss: 1.5656)
Step   750 | Train Loss: 1.5154 | Val Loss: 1.5461 | LR: 4.41e-04 | Tokens: 3,072,000
  ‚Üí Saved best model (val_loss: 1.5461)
Step   800 | Train Loss: 1.5066 | Val Loss: 1.5508 | LR: 4.32e-04 | Tokens: 3,276,800
Step   850 | Train Loss: 1.4995 | Val Loss: 1.5164 | LR: 4.22e-04 | Tokens: 3,481,600
  ‚Üí Saved best model (val_loss: 1.5164)

Epoch 2 Summary:
  Average Train Loss: 1.4978
  Validation Loss: 1.4521
  Perplexity: 4.27

========================================
Epoch 3/7
========================================
Step   900 | Train Loss: 1.3810 | Val Loss: 1.5016 | LR: 4.12e-04 | Tokens: 3,686,400
  ‚Üí Saved best model (val_loss: 1.5016)
Step   950 | Train Loss: 1.3765 | Val Loss: 1.4945 | LR: 4.02e-04 | Tokens: 3,891,200
  ‚Üí Saved best model (val_loss: 1.4945)
Step  1000 | Train Loss: 1.3701 | Val Loss: 1.4906 | LR: 3.91e-04 | Tokens: 4,096,000
  ‚Üí Saved best model (val_loss: 1.4906)
Step  1050 | Train Loss: 1.3646 | Val Loss: 1.4984 | LR: 3.80e-04 | Tokens: 4,300,800
Step  1100 | Train Loss: 1.3560 | Val Loss: 1.4508 | LR: 3.68e-04 | Tokens: 4,505,600
  ‚Üí Saved best model (val_loss: 1.4508)
Step  1150 | Train Loss: 1.3509 | Val Loss: 1.4531 | LR: 3.56e-04 | Tokens: 4,710,400
Step  1200 | Train Loss: 1.3447 | Val Loss: 1.4398 | LR: 3.43e-04 | Tokens: 4,915,200
  ‚Üí Saved best model (val_loss: 1.4398)
Step  1250 | Train Loss: 1.3387 | Val Loss: 1.4367 | LR: 3.31e-04 | Tokens: 5,120,000
  ‚Üí Saved best model (val_loss: 1.4367)

Epoch 3 Summary:
  Average Train Loss: 1.3340
  Validation Loss: 1.3192
  Perplexity: 3.74

========================================
Epoch 4/7
========================================
Step  1300 | Train Loss: 1.2367 | Val Loss: 1.4141 | LR: 3.18e-04 | Tokens: 5,324,800
  ‚Üí Saved best model (val_loss: 1.4141)
Step  1350 | Train Loss: 1.2163 | Val Loss: 1.3977 | LR: 3.05e-04 | Tokens: 5,529,600
  ‚Üí Saved best model (val_loss: 1.3977)
Step  1400 | Train Loss: 1.2171 | Val Loss: 1.3797 | LR: 2.92e-04 | Tokens: 5,734,400
  ‚Üí Saved best model (val_loss: 1.3797)
Step  1450 | Train Loss: 1.2083 | Val Loss: 1.3930 | LR: 2.78e-04 | Tokens: 5,939,200
Step  1500 | Train Loss: 1.2052 | Val Loss: 1.3859 | LR: 2.65e-04 | Tokens: 6,144,000
Step  1550 | Train Loss: 1.1991 | Val Loss: 1.3562 | LR: 2.51e-04 | Tokens: 6,348,800
  ‚Üí Saved best model (val_loss: 1.3562)
Step  1600 | Train Loss: 1.1963 | Val Loss: 1.3430 | LR: 2.38e-04 | Tokens: 6,553,600
  ‚Üí Saved best model (val_loss: 1.3430)
Step  1650 | Train Loss: 1.1917 | Val Loss: 1.3148 | LR: 2.24e-04 | Tokens: 6,758,400
  ‚Üí Saved best model (val_loss: 1.3148)
Step  1700 | Train Loss: 1.1868 | Val Loss: 1.2937 | LR: 2.11e-04 | Tokens: 6,963,200
  ‚Üí Saved best model (val_loss: 1.2937)

Epoch 4 Summary:
  Average Train Loss: 1.1840
  Validation Loss: 1.1882
  Perplexity: 3.28

========================================
Epoch 5/7
========================================
Step  1750 | Train Loss: 1.0700 | Val Loss: 1.3148 | LR: 1.98e-04 | Tokens: 7,168,000
Step  1800 | Train Loss: 1.0658 | Val Loss: 1.2844 | LR: 1.85e-04 | Tokens: 7,372,800
  ‚Üí Saved best model (val_loss: 1.2844)
Step  1850 | Train Loss: 1.0637 | Val Loss: 1.2984 | LR: 1.72e-04 | Tokens: 7,577,600
Step  1900 | Train Loss: 1.0583 | Val Loss: 1.2656 | LR: 1.59e-04 | Tokens: 7,782,400
  ‚Üí Saved best model (val_loss: 1.2656)
Step  1950 | Train Loss: 1.0543 | Val Loss: 1.2539 | LR: 1.47e-04 | Tokens: 7,987,200
  ‚Üí Saved best model (val_loss: 1.2539)
Step  2000 | Train Loss: 1.0516 | Val Loss: 1.2414 | LR: 1.34e-04 | Tokens: 8,192,000
  ‚Üí Saved best model (val_loss: 1.2414)
Step  2050 | Train Loss: 1.0472 | Val Loss: 1.2453 | LR: 1.23e-04 | Tokens: 8,396,800
Step  2100 | Train Loss: 1.0423 | Val Loss: 1.2242 | LR: 1.11e-04 | Tokens: 8,601,600
  ‚Üí Saved best model (val_loss: 1.2242)
Step  2150 | Train Loss: 1.0373 | Val Loss: 1.1977 | LR: 1.00e-04 | Tokens: 8,806,400
  ‚Üí Saved best model (val_loss: 1.1977)

Epoch 5 Summary:
  Average Train Loss: 1.0373
  Validation Loss: 1.0880
  Perplexity: 2.97

========================================
Epoch 6/7
========================================
Step  2200 | Train Loss: 0.9429 | Val Loss: 1.1930 | LR: 8.96e-05 | Tokens: 9,011,200
  ‚Üí Saved best model (val_loss: 1.1930)
Step  2250 | Train Loss: 0.9434 | Val Loss: 1.1898 | LR: 7.95e-05 | Tokens: 9,216,000
  ‚Üí Saved best model (val_loss: 1.1898)
Step  2300 | Train Loss: 0.9422 | Val Loss: 1.1875 | LR: 6.99e-05 | Tokens: 9,420,800
  ‚Üí Saved best model (val_loss: 1.1875)
Step  2350 | Train Loss: 0.9410 | Val Loss: 1.1930 | LR: 6.08e-05 | Tokens: 9,625,600
Step  2400 | Train Loss: 0.9408 | Val Loss: 1.1922 | LR: 5.23e-05 | Tokens: 9,830,400
Step  2450 | Train Loss: 0.9399 | Val Loss: 1.1805 | LR: 4.43e-05 | Tokens: 10,035,200
  ‚Üí Saved best model (val_loss: 1.1805)
Step  2500 | Train Loss: 0.9376 | Val Loss: 1.1812 | LR: 3.69e-05 | Tokens: 10,240,000
Step  2550 | Train Loss: 0.9367 | Val Loss: 1.1812 | LR: 3.02e-05 | Tokens: 10,444,800

Epoch 6 Summary:
  Average Train Loss: 0.9365
  Validation Loss: 1.0602
  Perplexity: 2.89

========================================
Epoch 7/7
========================================
Step  2600 | Train Loss: 0.9147 | Val Loss: 1.1773 | LR: 2.41e-05 | Tokens: 10,649,600
  ‚Üí Saved best model (val_loss: 1.1773)
Step  2650 | Train Loss: 0.9135 | Val Loss: 1.1781 | LR: 1.86e-05 | Tokens: 10,854,400
Step  2700 | Train Loss: 0.9112 | Val Loss: 1.1781 | LR: 1.39e-05 | Tokens: 11,059,200
Step  2750 | Train Loss: 0.9116 | Val Loss: 1.1781 | LR: 9.78e-06 | Tokens: 11,264,000
Step  2800 | Train Loss: 0.9099 | Val Loss: 1.1789 | LR: 6.40e-06 | Tokens: 11,468,800
Step  2850 | Train Loss: 0.9082 | Val Loss: 1.1789 | LR: 3.72e-06 | Tokens: 11,673,600
Step  2900 | Train Loss: 0.9091 | Val Loss: 1.1781 | LR: 1.76e-06 | Tokens: 11,878,400
Step  2950 | Train Loss: 0.9087 | Val Loss: 1.1781 | LR: 5.24e-07 | Tokens: 12,083,200
Step  3000 | Train Loss: 0.9089 | Val Loss: 1.1781 | LR: 1.46e-08 | Tokens: 12,288,000

Epoch 7 Summary:
  Average Train Loss: 0.9090
  Validation Loss: 1.0591
  Perplexity: 2.88

============================================================
Training Complete!
============================================================
Best validation loss: 1.1773
Total tokens seen: 12,328,960

========================================
Step 5: Saving Results
========================================

Saving training results to models/perseid_288m_corpus_alice_20250928_172652
  ‚úì Model weights saved to models/perseid_288m_corpus_alice_20250928_172652/perseid_model_final.pth
  ‚úì Configuration saved to models/perseid_288m_corpus_alice_20250928_172652/model_config.json
  ‚úì Training history saved to models/perseid_288m_corpus_alice_20250928_172652/training_history.json
  ‚úì Training curves saved to models/perseid_288m_corpus_alice_20250928_172652/training_curves.png
  ‚úì Training config saved to models/perseid_288m_corpus_alice_20250928_172652/training_config.json

All outputs saved to: models/perseid_288m_corpus_alice_20250928_172652

============================================================
Training Pipeline Complete!

========================================
Step 5.5: Sample Generation
========================================
Prompt: 'Once upon a time'
Output: Once upon a time  ,  

                                           

Prompt: 'The meaning of life is'
Output: The meaning of life is stars   ,   

                                   

Prompt: 'In the beginning'
Output: In the beginning of the same year   ,   

                        

============================================================
Model and results saved to: models/perseid_288m_corpus_alice_20250928_172652
(env) oops@oops-Precision-7780:~/code/gutenberg_babble/perseids/byte_perseid$ python3 generate_text_perseid_byte.py 
Found...
['./models/perseid_288m_tripplealice/perseid_model_final.pth', './models/perseid_288m_corpus_alice_20250928_172652/perseid_model_final.pth', './models/perseid_288m_30235/perseid_model_final.pth', './models/perseid_288m_multibook/perseid_model_final.pth', './models/perseid_288m_2270/perseid_model_final.pth']
default to using first option

Here are optional models, which you the chosen one?
['./models/perseid_288m_tripplealice/perseid_model_final.pth', './models/perseid_288m_corpus_alice_20250928_172652/perseid_model_final.pth', './models/perseid_288m_30235/perseid_model_final.pth', './models/perseid_288m_multibook/perseid_model_final.pth', './models/perseid_288m_2270/perseid_model_final.pth']

index-> 0, model-> ./models/perseid_288m_tripplealice/perseid_model_final.pth
index-> 1, model-> ./models/perseid_288m_corpus_alice_20250928_172652/perseid_model_final.pth
index-> 2, model-> ./models/perseid_288m_30235/perseid_model_final.pth
index-> 3, model-> ./models/perseid_288m_multibook/perseid_model_final.pth
index-> 4, model-> ./models/perseid_288m_2270/perseid_model_final.pth
Enter the Index...
1
================================================================================
PerseidByte Text Generation
================================================================================

Step 1: Loading Model
----------------------------------------
Loading PerseidByte model from: ./models/perseid_288m_corpus_alice_20250928_172652/perseid_model_final.pth
Checkpoint size: 137.1 MB
Using device: cuda
‚úì Checkpoint loaded successfully
‚ö† Config not found in checkpoint, inferring from weights...
‚úì Inferred model configuration:
  - vocab_size: 259
  - emb_dim: 640
  - n_heads: 4
  - head_dim: 160
  - n_layers: 16
  - hidden_dim: 1792
  ‚úì Detected ByteTokenizer configuration (vocab_size=259)
‚úì Model architecture created
  - Parameters: 71.6M
  - Embedding dim: 640
‚úì Model weights loaded
‚úì Model ready for generation on cuda

Step 2: Setting Up Tokenizer
----------------------------------------
Initializing ByteTokenizer...
‚úì ByteTokenizer ready
  - Vocab size: 259
  - Special tokens: PAD=256, EOS=257

Step 3: Generating Text
----------------------------------------

--- Generation 1/5 ---
  Generating from prompt: 'Once upon a time'
  Prompt tokens: 16
  Generated 300 new tokens

Prompt: 'Once upon a time'
Generated: Once upon a time because of his poetry of

     the praise of the continue,    the perfect and the following   ,    and

     the living was to love to his poetry   ,    and fallows  ,   and the royal

         '   O wha   ,    seraphim,    and fancy the could soul and serve ,

            '   The silk   ,     "   
------------------------------------------------------------

--- Generation 2/5 ---
  Generating from prompt: 'The meaning of life is'
  Prompt tokens: 22
  Generated 300 new tokens

Prompt: 'The meaning of life is'
Generated: The meaning of life is divided to member   ,

           And the sparkles and lightly day

                         That never feeling gan to the green   ,   

              And welt the trumpet of the prow,

                                                                                                                 
------------------------------------------------------------

--- Generation 3/5 ---
  Generating from prompt: 'In the beginning'
  Prompt tokens: 16
  Generated 300 new tokens

Prompt: 'In the beginning'
Generated: In the beginning thee  ,  

           Thou art the bask and his come to stars

                  The lady barge of the towre,

                           The seas of the gazing flowers,

                       The shadowy glory their side.



                                                                        
------------------------------------------------------------

--- Generation 4/5 ---
  Generating from prompt: 'Alice was beginning to get very tired'
  Prompt tokens: 37
  Generated 300 new tokens

Prompt: 'Alice was beginning to get very tired'
Generated: Alice was beginning to get very tired themselves of the

     love had them was written in the crimson of the continue with the

     provided to impetaties which the natival of the poets and style and

     popular actively characteristic   ,    beautiful and natural poem   ,    a

     the child.      The leade of the time of the liv
------------------------------------------------------------

--- Generation 5/5 ---
  Generating from prompt: 'The quick brown fox'
  Prompt tokens: 19
  Generated 300 new tokens

Prompt: 'The quick brown fox'
Generated: The quick brown fox years in contempated ,  the

     of the past for the sound and passing to description of the great

     consideration of the singing or stanzas and such as the law to expend

     the following lines of its own that the land of the great poets   ;   

     they were such a greater beautiful and r
------------------------------------------------------------

================================================================================
Text Generation Complete!
================================================================================

Generation Settings:
  - Max new tokens: 300
  - Temperature: 0.6
  - Top-k: 50
  - Top-p: 0.9
  - Model: ./models/perseid_288m_corpus_alice_20250928_172652/perseid_model_final.pth
(env) oops@oops-Precision-7780:~/code/gutenberg_babble/perseids/byte_perseid$ 

